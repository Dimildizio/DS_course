This folder contains reports and visualizations summarizing our findings and results.

The group projects has been successfully finished. We Got a best score of 0.12334 which is about 500 of current 4500 competitors. 

What hasn't been done: Could have used SVM in Stacking, it has showm remarkable results (and pretty bad if solo).

Didn't have neither time nor desire to work with CatBoost due to the architecture of the code we already had and difficulties implementing it, fitting it's hyperparameters (and choosing them) into a fully-written structure. Anyways, would be a good idea to study it at our own pace, if got time and mood to write a function for training and selecting a model from scratch. Judging by the results of the other teams - CatBoost is a slow but magical pill that does most of the work for you, so thats a must for learning ml models and should be on our todo list. It does increase results a bit, but not that much. Even though it works basically out of the box. One thing left is to use KFolds which might help as well since batches do inrease performance.
