{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJz2mAt5RJlLvO5XX+t8Nu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Seq2Seq/Seq2Seq_translation_HSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq tanslation"
      ],
      "metadata": {
        "id": "QvrD3bwUTyvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Today we shall compose encoder-decoder neural networks and apply them to the task of machine translation.\n",
        "\n",
        "![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n",
        "_(img: esciencegroup.files.wordpress.com)_\n",
        "\n",
        "\n",
        "Encoder-decoder architectures are about converting anything to anything, including\n",
        " * Machine translation and spoken dialogue systems\n",
        " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
        " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
        " * Grapheme2phoneme - convert words to transcripts"
      ],
      "metadata": {
        "id": "47E4XC9qT5th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our task: machine translation\n",
        "\n",
        "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions. This task shows the scale of machine translation while not requiring you to train your model for weeks if you don't use GPU.\n",
        "\n",
        "Before we get to the architecture, there's some preprocessing to be done. ~~Go tokenize~~ Alright, this time we've done preprocessing for you. As usual, the data will be tokenized with WordPunctTokenizer.\n",
        "\n",
        "However, there's one more thing to do. Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
        "\n",
        "One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "ueCCJnWUT_IY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0mhxxU4Ttwf",
        "outputId": "61c9eb1b-e065-467b-cf74-664c7d6cd1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-20 21:47:28--  https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/yy2zqh34dyhv07i/data.txt [following]\n",
            "--2024-03-20 21:47:28--  https://www.dropbox.com/s/dl/yy2zqh34dyhv07i/data.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com/cd/0/get/CPcPir-Rt_xowSc1Oebv-TejWTmGdDiw7o6W2c51P-1Rak9VHxaDm0ouFzfJLz7Ze6rfhAqBIlc5wAWvF-MGsza9Wfurkgn6fGzsNLlPC482axD-Z89_Y-AoaI6YsWWZzIs/file?dl=1# [following]\n",
            "--2024-03-20 21:47:29--  https://uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com/cd/0/get/CPcPir-Rt_xowSc1Oebv-TejWTmGdDiw7o6W2c51P-1Rak9VHxaDm0ouFzfJLz7Ze6rfhAqBIlc5wAWvF-MGsza9Wfurkgn6fGzsNLlPC482axD-Z89_Y-AoaI6YsWWZzIs/file?dl=1\n",
            "Resolving uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com (uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com (uc6d794a352a80af7ab4484ecc3d.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12905335 (12M) [application/binary]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "data.txt            100%[===================>]  12.31M  45.0MB/s    in 0.3s    \n",
            "\n",
            "2024-03-20 21:47:29 (45.0 MB/s) - ‘data.txt’ saved [12905335/12905335]\n",
            "\n",
            "--2024-03-20 21:47:29--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2879 (2.8K) [text/plain]\n",
            "Saving to: ‘vocab.py’\n",
            "\n",
            "vocab.py            100%[===================>]   2.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-20 21:47:29 (20.6 MB/s) - ‘vocab.py’ saved [2879/2879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch>=1.3.0\n",
        "!pip3 install subword-nmt &> log\n",
        "!wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py\n",
        "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "0R_vGzM6R-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sacrebleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from subword_nmt.learn_bpe import learn_bpe\n",
        "from subword_nmt.apply_bpe import BPE\n",
        "\n",
        "from vocab import Vocab\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QDlTJ9qsUZtK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "def tokenize(x):\n",
        "    return ' '.join(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "\n",
        "# split and tokenize the data\n",
        "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
        "    for line in open('data.txt'):\n",
        "        src_line, dst_line = line.strip().split('\\t')\n",
        "        f_src.write(tokenize(src_line) + '\\n')\n",
        "        f_dst.write(tokenize(dst_line) + '\\n')\n",
        "\n",
        "\n",
        "# build and apply bpe vocs\n",
        "bpe = {}\n",
        "for lang in ['en', 'ru']:\n",
        "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
        "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
        "\n",
        "    with open('train.bpe.' + lang, 'w') as f_out:\n",
        "        for line in open('train.' + lang):\n",
        "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4bg87sZUesG",
        "outputId": "ca092e45-79a9-488d-fb73-5accbbab46e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8000/8000 [00:30<00:00, 265.89it/s]\n",
            "100%|██████████| 8000/8000 [00:17<00:00, 466.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
      ],
      "metadata": {
        "id": "JD-jREptWAoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
        "data_out = np.array(open('./train.bpe.en').read().split('\\n'))"
      ],
      "metadata": {
        "id": "IVWhtVcIWF9L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=3000,\n",
        "                                                          random_state=42)\n",
        "for i in range(3):\n",
        "    print('inp:', train_inp[i])\n",
        "    print('out:', train_out[i], end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDBeixivVKaO",
        "outputId": "0287569b-3503-4ed9-d8dd-520b23b98c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inp: на территории обустроена бесплатная частная парковка .\n",
            "out: free private parking is available on site .\n",
            "\n",
            "inp: кроме того , в 5 минутах ходьбы работают многочисленные бары и рестораны .\n",
            "out: guests can find many bars and restaurants within a 5 - minute walk .\n",
            "\n",
            "inp: отель san mi@@ gu@@ el расположен в центре мор@@ ели@@ и , в 750 метрах от главной площади города и кафедрального собора .\n",
            "out: hotel san miguel is located in central more@@ lia , 750 metres from the city ’ s main square and cathedral .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp_voc = Vocab.from_lines(train_inp)\n",
        "out_voc = Vocab.from_lines(train_out)"
      ],
      "metadata": {
        "id": "xgRXsYuoVsJU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cwOoHfuhlrsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82aa0cbe-ec93-4a88-c421-53b460895e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lines\n",
            "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n",
            "\n",
            "words to ids (0 = bos, 1 = eos):\n",
            "tensor([[   0, 2688, 2943, 1108,   29,    1,    1,    1],\n",
            "        [   0, 2922, 1834, 8035,   59, 3800,   29,    1],\n",
            "        [   0, 6030, 2083,   29,    1,    1,    1,    1],\n",
            "        [   0, 4927, 1870,   29,    1,    1,    1,    1],\n",
            "        [   0, 5549, 1453,   27,  592,   29,    1,    1]])\n",
            "\n",
            "back to words\n",
            "['гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .', 'подключен wi - fi .']\n"
          ]
        }
      ],
      "source": [
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = sorted(train_inp, key=len)[5:10]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw source and translation length distributions to estimate the scope of the task."
      ],
      "metadata": {
        "id": "K-uW9POQWPhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[12, 3])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"source length\")\n",
        "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"translation length\")\n",
        "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "UKk6oTrrWQL9",
        "outputId": "15bfa139-66b7-4dcf-f3a4-f5885d1f44e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture"
      ],
      "metadata": {
        "id": "hrFuT2SnWchd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. This model is implemented for you as a reference and a baseline for your homework assignment."
      ],
      "metadata": {
        "id": "GdGCtPT4WeKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "4gOGLtqlWhre"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder seq2seq model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
        "\n",
        "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
        "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
        "\n",
        "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
        "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
        "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
        "\n",
        "    def forward(self, inp, out):\n",
        "        \"\"\" Apply model in training mode \"\"\"\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "\n",
        "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
        "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
        "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
        "        # ^-- shape: [batch_size, hid_size]\n",
        "\n",
        "        dec_start = self.dec_start(last_state)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]\n",
        "        \"\"\"\n",
        "        prev_gru0_state = prev_state[0]\n",
        "        prev_emb = self.emb_out(prev_tokens)\n",
        "\n",
        "        new_dec_state = self.dec0(prev_emb, prev_gru0_state)\n",
        "\n",
        "        output_logits = self.logits(new_dec_state)\n",
        "        return [new_dec_state], output_logits\n",
        "\n",
        "\n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
        "        batch_size = out_tokens.shape[0]\n",
        "        state = initial_state\n",
        "\n",
        "        # initial logits: always predict BOS\n",
        "        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),\n",
        "                               num_classes=len(self.out_voc)).to(device=out_tokens.device)\n",
        "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
        "\n",
        "        logits_sequence = [first_logits]\n",
        "        for i in range(out_tokens.shape[1] - 1):\n",
        "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
        "            logits_sequence.append(logits)\n",
        "        return torch.stack(logits_sequence, dim=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        \"\"\" Generate translations from model (greedy version) \"\"\"\n",
        "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
        "        state = initial_state\n",
        "        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,\n",
        "                              device=device)]\n",
        "        all_states = [initial_state]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(logits.argmax(dim=-1))\n",
        "            all_states.append(state)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), all_states\n",
        "\n",
        "    def translate_lines(self, inp_lines, **kwargs):\n",
        "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
        "        initial_state = self.encode(inp)\n",
        "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
        "        return self.out_voc.to_lines(out_ids.cpu().numpy()), states\n"
      ],
      "metadata": {
        "id": "ZbvDs3hPWpQ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bathe in tests"
      ],
      "metadata": {
        "id": "POiYnoh-lLi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# debugging area\n",
        "model = BasicModel(inp_voc, out_voc).to(device)\n",
        "\n",
        "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
        "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
        "\n",
        "h0 = model.encode(dummy_inp_tokens)\n",
        "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
        "\n",
        "assert isinstance(h1, list) and len(h1) == len(h0)\n",
        "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
        "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
        "\n",
        "logits_seq = model.decode(h0, dummy_out_tokens)\n",
        "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
        "\n",
        "# full forward\n",
        "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
        "assert logits_seq2.shape == logits_seq.shape\n",
        "\n",
        "print('All tests passed')\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV8UbrApayao",
        "outputId": "f3c88803-2a27-4a7a-8154-238521925385"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed\n",
            "BasicModel(\n",
            "  (emb_inp): Embedding(8048, 64)\n",
            "  (emb_out): Embedding(7801, 64)\n",
            "  (enc0): GRU(64, 128, batch_first=True)\n",
            "  (dec_start): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (dec0): GRUCell(64, 128)\n",
            "  (logits): Linear(in_features=128, out_features=7801, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)\n",
        "print(\"Translations without training:\")\n",
        "print('\\n'.join([line for line in dummy_translations]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tksqLYy9lZc6",
        "outputId": "38814c86-a4bb-43c0-9ee1-4bf710476660"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translations without training:\n",
            "pension yne seas@@ sque ment@@ bled sim minibar gand@@ santa foo@@ dubai pin@@ strand ventura place love temple magni@@ bus onal monum@@ ised poster river@@\n",
            "nang salmon lock@@ co@@ mical grand muse@@ after@@ aug@@ kovo kovo paulo roc@@ bois 8 8 pie@@ ala useful nightlife nightlife appliances nic 10@@ ebel@@\n",
            "pension yne seas@@ sque ment@@ bled sim minibar gand@@ santa foo@@ dubai pin@@ strand ventura place love temple magni@@ bus onal monum@@ ised poster river@@\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loss (2 points)\n",
        "\n",
        "Our training objective is almost the same as it was for neural language models:\n",
        "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
        "\n",
        "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
      ],
      "metadata": {
        "id": "mM5Iy2RVpI08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_loss(model, inp, out, f=False, **flags):\n",
        "    \"\"\"\n",
        "    Compute loss (float32 scalar) as in the formula above\n",
        "    :param inp: input tokens matrix, int32[batch, time]\n",
        "    :param out: reference tokens matrix, int32[batch, time]\n",
        "\n",
        "    In order to pass the tests, your function should\n",
        "    * include loss at first EOS but not the subsequent ones\n",
        "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
        "    \"\"\"\n",
        "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
        "    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
        "\n",
        "    initial_state = model.encode(inp)\n",
        "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
        "    logits_seq = model.decode(initial_state, out, **flags)\n",
        "\n",
        "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
        "    logprobs_seq = F.log_softmax(logits_seq, dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "    #log-probabilities of correct outputs, [batch_size, out_len]\n",
        "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)\n",
        "    # ^-- this will select the probability of the actual next token.\n",
        "\n",
        "    # Note: you can compute loss more efficiently using using F.cross_entropy\n",
        "    celoss = get_CE_loss(logprobs_seq, out, mask)\n",
        "\n",
        "    loss = -logp_out[mask].mean()\n",
        "    #average cross-entropy over tokens where mask == True\n",
        "    return loss if f else celoss # average loss, scalar\n",
        "\n",
        "\n",
        "def get_CE_loss(logp_seq, out, mask):\n",
        "    '''\n",
        "    Calculate CE loss using F function\n",
        "    '''\n",
        "    lp_seq_f = logp_seq.view(-1, logp_seq.size(-1))\n",
        "    targets_f = out.view(-1)\n",
        "    mask_f = mask.view(-1)\n",
        "    loss = F.cross_entropy(lp_seq_f[mask_f], targets_f[mask_f], reduction='mean')\n",
        "    return loss"
      ],
      "metadata": {
        "id": "MVZjGVcYzUnV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bathe in tests"
      ],
      "metadata": {
        "id": "Q5XQaPCu10mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
        "print(\"Loss:\", dummy_loss)\n",
        "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens, True)\n",
        "print(\"Loss:\", dummy_loss)\n",
        "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), \"We're sorry for your loss\"\n",
        "\n",
        "# test autograd\n",
        "dummy_loss.backward()\n",
        "for name, param in model.named_parameters():\n",
        "    assert param.grad is not None and abs(param.grad.max()) != 0, f\"Param {name} received no gradients\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaXc0uY50wuh",
        "outputId": "7a7e2cfb-5630-41fa-b4ae-230ef5ce5786"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(7.5347, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(7.5347, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: BLEU\n",
        "\n",
        "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
        "\n",
        "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
      ],
      "metadata": {
        "id": "A-UEqP4LQjsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
        "    \"\"\"\n",
        "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
        "    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
        "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
        "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
        "        return corpus_bleu(\n",
        "            [[ref.split()] for ref in actual],\n",
        "            [trans.split() for trans in translations],\n",
        "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
        "            ) * 100"
      ],
      "metadata": {
        "id": "shKmKAOcQncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_bleu(model, dev_inp, dev_out)"
      ],
      "metadata": {
        "id": "lf1L8X5DQp9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sacrebleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
        "    \"\"\"\n",
        "    Estimates corpora-level BLEU score of model's translations given inp and reference out.\n",
        "    Use sacrebleu library for a more serious BLEU computation.\n",
        "\n",
        "    :param model: The translation model.\n",
        "    :param inp_lines: List of input sentences to translate.\n",
        "    :param out_lines: List of reference translation sentences.\n",
        "    :param bpe_sep: Separator for subword units.\n",
        "    :param flags: Additional flags for model translation.\n",
        "    :return: BLEU score as computed by sacrebleu.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        translations, _ = model.translate_lines(inp_lines, **flags)\n",
        "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
        "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
        "\n",
        "        # Note: sacrebleu expects a list of translations and a list of lists of references\n",
        "        # Each reference should be wrapped in a list even if there is only one reference per input\n",
        "        refs = [[ref] for ref in actual]\n",
        "\n",
        "        bleu = sacrebleu.corpus_bleu(translations, refs)\n",
        "        return bleu.score"
      ],
      "metadata": {
        "id": "ZUR2varoRVsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_sacrebleu(model, dev_inp, dev_out)"
      ],
      "metadata": {
        "id": "jKYMO8vXSluo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}