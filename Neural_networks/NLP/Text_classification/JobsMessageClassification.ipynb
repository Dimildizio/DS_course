{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Text_classification/JobsMessageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ],
      "metadata": {
        "id": "wlZOb4Dl-Dhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk\n",
        "!pip install emoji --upgrade\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "alxSWiApUsUd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iF4iz0A48-vN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "import re\n",
        "import emoji\n",
        "import string\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, reuters\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pymystem3 import Mystem\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download(\"reuters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrXVzNn0UzEC",
        "outputId": "c850a436-36d9-4f35-ad4e-5a525b2a48f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze seeds"
      ],
      "metadata": {
        "id": "Adr7nZLtTrH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "SeqZPHCETtge"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the dataset"
      ],
      "metadata": {
        "id": "5Bc_Iah-_hm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = pd.read_excel('msg_type.xlsx')"
      ],
      "metadata": {
        "id": "GcSsofsF-dMV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "orbCX_lh-Wj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()"
      ],
      "metadata": {
        "id": "PCpZOGIf-Wy0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "Qz286hbW-pdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"russian\")\n",
        "stemmer_eng = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "Eau245xn_Q4N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmaization"
      ],
      "metadata": {
        "id": "gQ-tCd_W_r8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mystem = Mystem()"
      ],
      "metadata": {
        "id": "rAdp3V8i_tpg",
        "outputId": "904e08df-3206-475a-d50f-311180973660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_eng.stem('python')#.analyze('python')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MRhaVWbxfLoR",
        "outputId": "32ebfaf6-7e8e-4850-c63b-926e85fa9557"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'python'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize using TFIDF"
      ],
      "metadata": {
        "id": "dXTCd6Lb_3DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('russian'))"
      ],
      "metadata": {
        "id": "9bS02FX5_5F0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset to parameters and encode target labels"
      ],
      "metadata": {
        "id": "nyQPCc1bC7Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = text_data.copy()\n",
        "#df['category'] = df['category'].replace({'ads': 'message', 'project': 'vacancy'})"
      ],
      "metadata": {
        "id": "9VEDUqDtC-jq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category']"
      ],
      "metadata": {
        "id": "Y6hVYWNpv979",
        "outputId": "bfb62ad3-3015-4b37-f931-03cd9f287163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          ads\n",
              "1          ads\n",
              "2          ads\n",
              "3          ads\n",
              "4          ads\n",
              "        ...   \n",
              "475    vacancy\n",
              "476    vacancy\n",
              "477    vacancy\n",
              "478    vacancy\n",
              "479    vacancy\n",
              "Name: category, Length: 480, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Label encode categories"
      ],
      "metadata": {
        "id": "PdmxZUdeVnlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_target = label_encoder.fit_transform(df['category'])"
      ],
      "metadata": {
        "id": "hzhggYegVrIY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into params and target values"
      ],
      "metadata": {
        "id": "wRqIDsNJV4Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = encoded_target"
      ],
      "metadata": {
        "id": "e0-QiBASVlxW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform transformation on df"
      ],
      "metadata": {
        "id": "egQ65yKOCyux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text: str) -> str:\n",
        "    return emoji.replace_emoji(text, \" \")\n",
        "\n",
        "\n",
        "def remove_links(text: str) -> str:\n",
        "    return re.sub(r\"http\\S+\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "def remove_usernames_and_emails(text: str) -> str:\n",
        "    \"\"\"Удалеяет юзернеймы и email\"\"\"\n",
        "    return re.sub(r\"\\S*@\\S*\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    \"\"\"Удаляем символы пунктуации\"\"\"\n",
        "    return \"\".join([ch if ch not in string.punctuation else \" \" for ch in text])\n",
        "\n",
        "\n",
        "def remove_numbers(text: str) -> str:\n",
        "    \"\"\"Удаляем числа\"\"\"\n",
        "    return \"\".join([i if not i.isdigit() else \" \" for i in text])\n",
        "\n",
        "\n",
        "def remove_multiple_spaces(text: str) -> str:\n",
        "    \"\"\"Удаляем двойные (и более) пробелы\"\"\"\n",
        "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)"
      ],
      "metadata": {
        "id": "kvAuLGg2DeLL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prep_text(text: str) -> str:\n",
        "  return remove_multiple_spaces(\n",
        "      remove_numbers(\n",
        "          remove_punctuation(\n",
        "              remove_usernames_and_emails(\n",
        "                  remove_links(\n",
        "                      remove_emoji(text)\n",
        "                      )\n",
        "                  )\n",
        "              )\n",
        "          )\n",
        "      )"
      ],
      "metadata": {
        "id": "Ze1Dd5irEFH8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBR1DSgUWjqw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#our new dataset with stemmed lemmatized and later vectorized texts\n",
        "stemmed_lemma_txts = []\n",
        "\n",
        "for text in X:\n",
        "  tok = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "  stem_tok = [stemmer.stem(token) for token in tok]\n",
        "  # lem_tok = [lem for lem in mystem.lemmatize(\" \".join(stem_tok)) if not lem.isspace()]\n",
        "  # stemmed_lemma_txts.append(' '.join(lem_tok))\n",
        "  stemmed_lemma_txts.append(' '.join(stem_tok))\n",
        "\n",
        "df['text_lemm'] = stemmed_lemma_txts"
      ],
      "metadata": {
        "id": "_AoklUd1CyQu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF Vectorize"
      ],
      "metadata": {
        "id": "ao8AJx7yD188"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfd = tfidf_vectorizer.fit_transform(stemmed_lemma_txts)"
      ],
      "metadata": {
        "id": "TPetowxrD0BS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset"
      ],
      "metadata": {
        "id": "EPEBSiRxAGHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pass tfidf'd and transfromed data instead of texts as X"
      ],
      "metadata": {
        "id": "pS8tzS8AVK66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidfd, y, stratify=y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "3FHZ9SUyAHx-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7HX96oveEee1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and train baseline model"
      ],
      "metadata": {
        "id": "ZshtoMLyAgcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C=0.004)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JzH_z-mPADkt",
        "outputId": "40fbe3db-cbdd-4054-8b53-a90bbd1f9966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.004)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.004)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.004)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "Sy2G_blXEcE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-gapxEVgAfrY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "n1Dd5275EjjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "bS6NmFkhEvw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ],
      "metadata": {
        "id": "UGsDgjo8El3O",
        "outputId": "f02248a1-c7f9-4bdb-995e-310f1ec0b1b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report"
      ],
      "metadata": {
        "id": "bC_2nOceExNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "G3_u5H3RE2ne",
        "outputId": "560165b9-3805-4dbc-bd3f-014a9fa7da29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.88      0.73      0.80        30\n",
            "           3       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.85      0.85      0.85       120\n",
            "weighted avg       0.85      0.85      0.85       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sum it up"
      ],
      "metadata": {
        "id": "QJXTDgm66jBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Catboost"
      ],
      "metadata": {
        "id": "l75Q3u20xSLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_model = CatBoostClassifier(iterations=400, depth=6, learning_rate=0.04, loss_function='MultiClass', verbose=False)\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRD3V3L5puaX",
        "outputId": "aecaa330-e540-4444-94c7-d82ba50dae08"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7fc4b537fbe0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cat_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "class_names = ['ads','message', 'project', 'vacancy']\n",
        "report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB79wrrDsBmo",
        "outputId": "8538b569-d1f1-4c46-be62-cd35918aa60b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.82\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.91      0.67      0.77        30\n",
            "     message       0.74      0.97      0.84        30\n",
            "     project       0.81      0.73      0.77        30\n",
            "     vacancy       0.84      0.90      0.87        30\n",
            "\n",
            "    accuracy                           0.82       120\n",
            "   macro avg       0.83      0.82      0.81       120\n",
            "weighted avg       0.83      0.82      0.81       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model):\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print('Accuracy:', acc)\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Y_7ouuCQ6iXJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_model = LogisticRegression(C=0.004)\n",
        "nb_model = MultinomialNB()\n",
        "svm_model = SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "                              ('rf', rf_model),\n",
        "                              ('svm', svm_model),\n",
        "                              ('xgb', xgb_model)],\n",
        "                            voting='soft')        # soft for probability-based voting\n",
        "\n",
        "rfensemble = VotingClassifier(estimators=[\n",
        "                              ('rf', rf_model),\n",
        "                              ('svm', svm_model),\n",
        "                              ('rf1', RandomForestClassifier(n_estimators=100, random_state=42))],\n",
        "                            voting='hard')\n",
        "\n",
        "models = [log_model, nb_model, svm_model, rf_model, xgb_model, knn_model, ensemble,rfensemble]\n",
        "model_names = ['logreg', 'bayes', 'SVM', 'RandomForest', 'XGB', 'KNN', 'Ensemble', 'RF_ensemble']"
      ],
      "metadata": {
        "id": "AC9Hns0G68Qs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  print(model_names[i])\n",
        "  print()\n",
        "  test_model(models[i])\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "id": "TYOo_QZ98p01",
        "outputId": "980c91e8-29ac-49f3-ad3c-3a69b6a74f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logreg\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.88      0.73      0.80        30\n",
            "           3       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.85      0.85      0.85       120\n",
            "weighted avg       0.85      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bayes\n",
            "\n",
            "Accuracy: 0.775\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.90      0.79        30\n",
            "           1       0.89      0.53      0.67        30\n",
            "           2       0.70      0.70      0.70        30\n",
            "           3       0.85      0.97      0.91        30\n",
            "\n",
            "    accuracy                           0.78       120\n",
            "   macro avg       0.79      0.78      0.77       120\n",
            "weighted avg       0.79      0.78      0.77       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SVM\n",
            "\n",
            "Accuracy: 0.8583333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81        30\n",
            "           1       0.78      0.93      0.85        30\n",
            "           2       0.89      0.80      0.84        30\n",
            "           3       0.96      0.90      0.93        30\n",
            "\n",
            "    accuracy                           0.86       120\n",
            "   macro avg       0.86      0.86      0.86       120\n",
            "weighted avg       0.86      0.86      0.86       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RandomForest\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.81        30\n",
            "           1       0.74      0.97      0.84        30\n",
            "           2       0.83      0.83      0.83        30\n",
            "           3       0.93      0.90      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.87      0.85      0.85       120\n",
            "weighted avg       0.87      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "XGB\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.73      0.81        30\n",
            "           1       0.78      0.97      0.87        30\n",
            "           2       0.88      0.77      0.82        30\n",
            "           3       0.85      0.93      0.89        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.86      0.85      0.85       120\n",
            "weighted avg       0.86      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "KNN\n",
            "\n",
            "Accuracy: 0.7333333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.77      0.69        30\n",
            "           1       0.95      0.70      0.81        30\n",
            "           2       0.68      0.70      0.69        30\n",
            "           3       0.77      0.77      0.77        30\n",
            "\n",
            "    accuracy                           0.73       120\n",
            "   macro avg       0.76      0.73      0.74       120\n",
            "weighted avg       0.76      0.73      0.74       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ensemble\n",
            "\n",
            "Accuracy: 0.8833333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.73      0.83        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.90      0.87      0.88        30\n",
            "           3       0.91      0.97      0.94        30\n",
            "\n",
            "    accuracy                           0.88       120\n",
            "   macro avg       0.89      0.88      0.88       120\n",
            "weighted avg       0.89      0.88      0.88       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RF_ensemble\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.81        30\n",
            "           1       0.74      0.97      0.84        30\n",
            "           2       0.83      0.83      0.83        30\n",
            "           3       0.93      0.90      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.87      0.85      0.85       120\n",
            "weighted avg       0.87      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.inverse_transform([0, 1])"
      ],
      "metadata": {
        "id": "FXRPVw4xjZSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8ee11b-32eb-47d9-b2b5-d26d017dbe86"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ads', 'message'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "kECINKJMZ4Y5",
        "outputId": "474725f0-11d1-4897-d6d5-6eef2b921371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ищу экспертов, кто хочет продавать свои услуги быстро и легко 🔥\\n\\nПривет, я Олеся - графический и веб-дизайнер экспертов. Я создаю лаконичные, с правильной структурой и собственным вайбом сайты для экспертов на Taplink. \\n\\nВся нужная и важная информация собрана в одном месте, чтобы ваш клиент проходил минимальный путь к покупке 💸\\n\\nЧтобы узнать подробнее пиши в лс \"СОТРУДНИЧЕСТВО\"🤍'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Element"
      ],
      "metadata": {
        "id": "2ViCtPXXZXzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[50]"
      ],
      "metadata": {
        "id": "6sDQJPRAaC7H",
        "outputId": "130015f4-41dc-40dc-d0fe-54fb55d32995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Ищешь копирайтера и PR специалиста? \\n\\nПривет...\n",
              "category                                                   ads\n",
              "text_lemm    ищеш копирайтер и pr специалист привет мен зов...\n",
              "Name: 50, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mywords = []\n",
        "text = df['text'][50]#'Продам гараж. Мопед не мой. Я просто разместил объяву.'\n",
        "tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "stk = [stemmer.stem(token) for token in tk]\n",
        "lmtk = [lem for lem in mystem.lemmatize(\" \".join(stk)) if not lem.isspace()]\n",
        "mywords.append(' '.join(lmtk))\n",
        "wrd = tfidf_vectorizer.transform(mywords)\n",
        "rf_model.predict(wrd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1vw2EZ1WsaZ",
        "outputId": "444e5e22-4776-435f-bf28-0777a9ba1548"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add Stacking of models"
      ],
      "metadata": {
        "id": "qQe_zR4VkwO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True)),\n",
        "     ('logreg', LogisticRegression(C=0.0045)),\n",
        "    ('xgb', xgb.XGBClassifier())]\n",
        "final_model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42)\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=final_model)\n",
        "test_model(stacking_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANjUw6m3hK1Q",
        "outputId": "fc20a613-825c-4f0a-bbb6-8c9ef9ff55e5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.875\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        30\n",
            "           1       0.85      0.93      0.89        30\n",
            "           2       0.86      0.80      0.83        30\n",
            "           3       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.88       120\n",
            "   macro avg       0.88      0.88      0.87       120\n",
            "weighted avg       0.88      0.88      0.87       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement as pipeline"
      ],
      "metadata": {
        "id": "kSmrpEmZidDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iPipe:\n",
        "  def __init__(self, estimator):\n",
        "    self.estimator = estimator\n",
        "    self.model = None\n",
        "\n",
        "  def prepipe(self, text):\n",
        "    tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "    stk = [stemmer.stem(token) for token in tk]\n",
        "    return ' '.join(stk)\n",
        "\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    self.model = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(preprocessor=self.prepipe)),\n",
        "        ('clf', self.estimator)])\n",
        "\n",
        "    self.model.fit(X_train, y_train)\n",
        "    return self.model\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.model.predict(x)\n",
        "\n",
        "\n",
        "  def evaluate(self, X_test, y_test):\n",
        "    y_pred = self.predict(X_test)\n",
        "    print('predicted')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "  def fit_predict(self, df):\n",
        "    X = df['text']\n",
        "    y = df['category']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
        "    self.train(X_train, y_train)\n",
        "    print('trained')\n",
        "    self.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "khJv0PCXe7hi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = iPipe(RandomForestClassifier(n_estimators=100, random_state=42, max_depth=6))\n",
        "pipe.fit_predict(text_data)"
      ],
      "metadata": {
        "id": "112mOgwohPuE",
        "outputId": "300c0f31-b0a6-4ece-d66e-8118052d2eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trained\n",
            "predicted\n",
            "Accuracy: 0.84375\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.94      0.71      0.81        24\n",
            "     message       0.75      1.00      0.86        24\n",
            "     project       0.89      0.71      0.79        24\n",
            "     vacancy       0.85      0.96      0.90        24\n",
            "\n",
            "    accuracy                           0.84        96\n",
            "   macro avg       0.86      0.84      0.84        96\n",
            "weighted avg       0.86      0.84      0.84        96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrd = ['Для мегастартапа новый гугл ищу датасатаниста. \\\n",
        "        плачу 300000 баксов\\наносек. надо все и сразу - подготовка, обработка, написание моделей, \\\n",
        "        создание красивеньких графиков и продажа данных. Нужен мидл с запросами джуна и компетенциями сеньора. \\\n",
        "        тимлид и техлид обязанности обязательно. возможно придется искать клиентов, презентовать им работу и вести \\\n",
        "        документации предприятия. Иногда мыть полы. Не задаваться вопросом зачем я нужен в проекте.']\n",
        "pipe.predict(wrd)"
      ],
      "metadata": {
        "id": "AjHf5Hnxiihw",
        "outputId": "d01ad3e9-69eb-4e8a-bc7a-b723478791a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['project'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Word2Vec"
      ],
      "metadata": {
        "id": "hXKBmxgNoKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "T9jkxooNoMMO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "metadata": {
        "id": "aQzkmraBoOZG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#api.info()\n",
        "word2vec_model = api.load('word2vec-ruscorpora-300')"
      ],
      "metadata": {
        "id": "acgU98cMosMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ea9213-eae3-460f-a48f-77f192f3e59d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 198.8/198.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convlist='''A       ADJ\n",
        "ADV     ADV\n",
        "ADVPRO  ADV\n",
        "ANUM    ADJ\n",
        "APRO    DET\n",
        "COM     ADJ\n",
        "CONJ    SCONJ\n",
        "INTJ    INTJ\n",
        "NONLEX  X\n",
        "NUM     NUM\n",
        "PART    PART\n",
        "PR      ADP\n",
        "S       NOUN\n",
        "SPRO    PRON\n",
        "UNKN    X\n",
        "V       VERB'''.split()\n",
        "conv_dict = {}\n",
        "for i in range(0, len(convlist), 2):\n",
        "    keyword = convlist[i]\n",
        "    value = convlist[i + 1]\n",
        "    conv_dict[keyword] = value\n"
      ],
      "metadata": {
        "id": "-jY_5RDkbYWV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag(word='пожар'):\n",
        "    processed = mystem.analyze(word)[0]\n",
        "    if not ('analysis' in processed):\n",
        "      return 'unknown'\n",
        "    elif not processed['analysis']:\n",
        "      return word\n",
        "    lemma = processed[\"analysis\"][0][\"lex\"].lower().strip()\n",
        "    pos = processed[\"analysis\"][0][\"gr\"].split(',')[0]\n",
        "    pos = pos.split('=')[0].strip()\n",
        "    tagged = lemma+'_'+conv_dict[pos]\n",
        "\n",
        "    return tagged"
      ],
      "metadata": {
        "id": "sWIitHrXZ0l3"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag('привет') in word2vec_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srzdly2GZPjI",
        "outputId": "f84bc122-aa0c-4c07-fb5a-2098d71dbef8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'lex': 'привет', 'wt': 1, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag('cpfsdm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0ZU6pZWEKXTK",
        "outputId": "62a65eeb-e3c3-45a1-849e-b47865fd90ec"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unknown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.key_to_index['искать_VERB']"
      ],
      "metadata": {
        "id": "VTjI5RBWzBD9",
        "outputId": "78db3068-6033-4a79-9133-5ef21ac94ff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "368"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = df['text'][0].split()[0]\n",
        "print(f'\"{w0}\" in corpora: {w0 in word2vec_model}')\n",
        "print(f'tag \"{tag(w0)}\" in corpora: {tag(w0) in word2vec_model}')"
      ],
      "metadata": {
        "id": "5FPXBrhdz_Os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4181f326-5c61-45dd-b52e-d1130358c3ca"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Ищу\" in corpora: False\n",
            "[{'lex': 'искать', 'wt': 1, 'gr': 'V,несов,пе=непрош,ед,изъяв,1-л'}]\n",
            "[{'lex': 'искать', 'wt': 1, 'gr': 'V,несов,пе=непрош,ед,изъяв,1-л'}]\n",
            "tag \"искать_VERB\" in corpora: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Might need to try and train w2v from zero or solve the _VERB _NOUN shyte of rus corpora"
      ],
      "metadata": {
        "id": "DC3mBdCv2PK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPD: No need. converting the word using stemma into base form and adding parts of speech type (with conversion) worked"
      ],
      "metadata": {
        "id": "ThJAaWIYdUlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add english corpora for english words"
      ],
      "metadata": {
        "id": "dNXOi6P0iPKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_mbeddings = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf7R-mooiRzk",
        "outputId": "fe9f8ceb-8562-4cfd-dd7b-0e35587e21d6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(en_mbeddings['unknown'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbVQLQzbiZPN",
        "outputId": "f32a1d04-e6c4-40f4-f075-e9392a2ab2c3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.2751107837684685"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create custom class for using word2vec in pipeline"
      ],
      "metadata": {
        "id": "QwXbaKnDqHml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecTrans(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, word2vec_model):\n",
        "        self.word2vec_model = word2vec_model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        embeddings = []\n",
        "        for text in X:\n",
        "            tokens = text.split()  # Split text into tokens\n",
        "            text_embeddings = []\n",
        "            for token in tokens:\n",
        "                token = tag(token)\n",
        "                if token in self.word2vec_model:\n",
        "                    text_embeddings.append(self.word2vec_model[token])\n",
        "                elif token in en_mbeddings:\n",
        "                    text_embeddings.append(en_mbeddings[token])\n",
        "            embeddings.append(text_embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "MRlLNdlvpXG7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create custom class for preprocessing stuff before pass it to word2vec"
      ],
      "metadata": {
        "id": "U-hU5TFhtUij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [self.preprocessor(text) for text in X]\n"
      ],
      "metadata": {
        "id": "oIFcgZnCtaSi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "  return ' '.join(tk)"
      ],
      "metadata": {
        "id": "hVI1Q6h1tjNa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a separate class for w2v pipeline"
      ],
      "metadata": {
        "id": "XtMYV3M1uYlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class W2VPipe(iPipe):\n",
        "  def __init__(self, preprocessor, w2v, estimator):\n",
        "    super().__init__(estimator)\n",
        "    self.preprocessor = preprocessor\n",
        "    self.w2v = w2v\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    self.model = Pipeline([\n",
        "        #('preprocess', self.preprocessor),\n",
        "        #('w2v', self.w2v),\n",
        "        ('clf', self.estimator)])\n",
        "\n",
        "    self.model.fit(X_train, y_train)\n",
        "    return self.model\n",
        "\n",
        "  def fit_predict(self, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
        "    self.train(X_train, y_train)\n",
        "    print('trained')\n",
        "    self.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "iIXtRaJ1qN7R"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gather it all together"
      ],
      "metadata": {
        "id": "r2ayrO_DvYlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.vector_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be-QnFmXUW_W",
        "outputId": "7681bb42-df08-45ae-f278-36594893bc1c"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_transformer = Word2VecTrans(word2vec_model)\n",
        "preprop = Preprocessor(preprocessor=preprocess)\n",
        "svm_w2v = SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True, C=5000)"
      ],
      "metadata": {
        "id": "v3hkLLj9rBRR"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = w2v_transformer.transform(preprop.transform(text_data['text']))"
      ],
      "metadata": {
        "id": "D8-OhE4wwVB2"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wmaxlen = max(len(x) for x in w2v)\n",
        "for entry in w2v:\n",
        "  while len(entry) < wmaxlen:\n",
        "    entry.append(0.0)\n",
        "avg_w2v = [np.mean(emb, axis=0) if emb else np.zeros(word2vec_model.vector_size) for emb in w2v]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gAqXDzURFXH",
        "outputId": "0003d19d-1def-4848-a616-6d8164de4be9"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asanyarray(a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wX_train, wX_test, wy_train, wy_test = train_test_split(avg_w2v, text_data['category'], random_state=42, stratify=y, test_size=0.2)"
      ],
      "metadata": {
        "id": "X8VFuSVBQ8JB"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wsvm = svm_w2v.fit(wX_train, wy_train)\n",
        "wy_pred = svm_w2v.predict(wX_test)\n",
        "waccuracy = accuracy_score(wy_test, wy_pred)\n",
        "print(\"Accuracy:\", waccuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(wy_test, wy_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek9u8F8ZSFbY",
        "outputId": "e560fc37-c39a-4050-f8bb-18965699d384"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84375\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.82      0.75      0.78        24\n",
            "     message       0.88      0.96      0.92        24\n",
            "     project       0.87      0.83      0.85        24\n",
            "     vacancy       0.80      0.83      0.82        24\n",
            "\n",
            "    accuracy                           0.84        96\n",
            "   macro avg       0.84      0.84      0.84        96\n",
            "weighted avg       0.84      0.84      0.84        96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add fastText for multilanguage word2vec"
      ],
      "metadata": {
        "id": "z2EFW633psUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")"
      ],
      "metadata": {
        "id": "Z9lYuY4f1uhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d338b994-c82a-49e7-fd88-768dc5520470"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 958.1/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_transformer = Word2VecTrans(fasttext_model)\n",
        "ftpreprop = Preprocessor(preprocessor=preprocess)"
      ],
      "metadata": {
        "id": "pJkSEoVMbhTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft = w2v_transformer.transform(preprop.transform(text_data['text']))\n",
        "ftmaxlen = max(len(x) for x in ft)\n",
        "for entry in ft:\n",
        "  while len(entry) < ftmaxlen:\n",
        "    entry.append(np.zeros(fasttext_model.vector_size))\n",
        "favg_w2v = [np.mean(emb, axis=0) if emb else np.zeros(fasttext_model.vector_size) for emb in ft]\n",
        "ftX_train, ftX_test, fty_train, fty_test = train_test_split(favg_w2v, text_data['category'], random_state=42, stratify=y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "_qFW-AYtdwE6"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_ft = SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True, C=6000)\n",
        "ft_forest = RandomForestClassifier(max_depth=5, random_state=42, n_estimators=100)"
      ],
      "metadata": {
        "id": "FtnZxHfoeAB7"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_ft in (svm_ft, ft_forest):\n",
        "  wsvm = model_ft.fit(ftX_train, fty_train)\n",
        "  fty_pred = model_ft.predict(ftX_test)\n",
        "  ftaccuracy = accuracy_score(fty_test, fty_pred)\n",
        "  print(\"Accuracy:\", ftaccuracy)\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report(fty_test, fty_pred))"
      ],
      "metadata": {
        "id": "NxXVFpjweOWD",
        "outputId": "2dbf3051-ea6b-4933-a550-d0dfb7ebfefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84375\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.82      0.75      0.78        24\n",
            "     message       0.88      0.96      0.92        24\n",
            "     project       0.87      0.83      0.85        24\n",
            "     vacancy       0.80      0.83      0.82        24\n",
            "\n",
            "    accuracy                           0.84        96\n",
            "   macro avg       0.84      0.84      0.84        96\n",
            "weighted avg       0.84      0.84      0.84        96\n",
            "\n",
            "Accuracy: 0.75\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.71      0.62      0.67        24\n",
            "     message       0.87      0.83      0.85        24\n",
            "     project       0.69      0.75      0.72        24\n",
            "     vacancy       0.73      0.79      0.76        24\n",
            "\n",
            "    accuracy                           0.75        96\n",
            "   macro avg       0.75      0.75      0.75        96\n",
            "weighted avg       0.75      0.75      0.75        96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural networks solution"
      ],
      "metadata": {
        "id": "sTYi4nuJB2A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import NN stuff"
      ],
      "metadata": {
        "id": "TMCtBxEsJFvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.utils import pad_sequences, to_categorical"
      ],
      "metadata": {
        "id": "RAOb_m7gIVf9"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "LgJOFcxcB5gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerocessing"
      ],
      "metadata": {
        "id": "jzuTUm2CCBVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data"
      ],
      "metadata": {
        "id": "1POaf7K8C-Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "EuLXF6KhM_hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_process(df):\n",
        "    X = df['text'].copy()\n",
        "    y = df['category'].copy()\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    return train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BxG3kH1UCCny"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize"
      ],
      "metadata": {
        "id": "3d_JVJ1NCXF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_tokenize(x1, x2):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x1)\n",
        "\n",
        "  x_train = tokenizer.texts_to_sequences(x1)\n",
        "  x_test = tokenizer.texts_to_sequences(x2)\n",
        "\n",
        "  return tokenizer, x_train, x_test"
      ],
      "metadata": {
        "id": "pzYvag2QCR0A"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add padding"
      ],
      "metadata": {
        "id": "th0cjU-yDWQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_padme(x1, x2, maxlen=100):\n",
        "  x_train = pad_sequences(x1, maxlen=maxlen, padding='post')\n",
        "  x_test = pad_sequences(x2, maxlen=maxlen, padding='post')\n",
        "  return x_train, x_test"
      ],
      "metadata": {
        "id": "_Gh0rzlmDXsC"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create model"
      ],
      "metadata": {
        "id": "55tAGeDeDoy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier:\n",
        "  def __init__(self, vocab_size, embedding_dim, maxlen):\n",
        "    self.vocab = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.maxlen = maxlen\n",
        "    self.model = self._create_model()\n",
        "\n",
        "  def _create_model(self)->Sequential:\n",
        "    '''creates lstm with embedding, rnn and classificator'''\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=self.vocab, output_dim=self.embedding_dim, input_length=self.maxlen))\n",
        "    model.add(LSTM(units=64))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "  def fit(self, X_train:np.array, y_train:np.array, batch_size:int=64, epochs:int=5)->None:\n",
        "    '''trains model'''\n",
        "\n",
        "    self.model.fit(X_train, y_train.astype(int), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  def predict(self, X_test:np.array)->np.array:\n",
        "    '''predics values for validation and test'''\n",
        "    print(\"PREDICTING\")\n",
        "    X_test_padded = pad_sequences(X_test, maxlen=self.maxlen)\n",
        "    return self.model.predict(X_test_padded)\n",
        "\n",
        "\n",
        "  def evaluate(self, pred:np.array, y_test:np.array)->None:\n",
        "    '''reports statistics'''\n",
        "    loss, accuracy = self.model.evaluate(pred, y_test)\n",
        "    pred = (pred > 0.5).astype(int)\n",
        "    print(f\"Loss: {loss}\\nAccuracy: {accuracy}\")\n",
        "    print(classification_report(y_test, pred))\n",
        "\n",
        "\n",
        "  def fit_predict(self, X_train, X_test, y_train, y_test, batch=64, epochs=5):\n",
        "    '''fits data into model, predicts values and reports sttistics'''\n",
        "\n",
        "    self.fit(X_train, y_train, batch, epochs)\n",
        "    y_pred = self.predict(X_test)\n",
        "    print('\\n\\n\\n\\n\\ntest binary\\n\\n\\n\\n')\n",
        "    # Convert predicted values to binary values (0 or 1)\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    self.evaluate(y_pred_binary, y_test)"
      ],
      "metadata": {
        "id": "dnGMRpJ9Edxe"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run it"
      ],
      "metadata": {
        "id": "CaDxAODHJMOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kX_train, kX_test, ky_train, ky_test = k_process(df)\n",
        "k_tokenizer, kX_train, kX_test = k_tokenize(kX_train, kX_test)\n",
        "kX_train, kXtest = k_padme(kX_train, kX_test)\n",
        "\n",
        "kX_train = np.array(kX_train)\n",
        "kX_test = np.array(kX_test)\n",
        "ky_train = np.array(ky_train)\n",
        "ky_test = np.array(ky_test)\n"
      ],
      "metadata": {
        "id": "aFuZLDqCJV8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca6e33a-9237-4e51-8a17-28ffbec58f43"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-171-47ef54b04943>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  kX_test = np.array(kX_test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_vocab_size = len(k_tokenizer.word_index) + 1    # num of unique words in all texts +1  for padding token\n",
        "k_embedding_dim = 100                             #vectors representing a word\n",
        "maxlen = 100                                      # sequence length after tokenization. too long - cut it"
      ],
      "metadata": {
        "id": "-xa12nSYJQ8u"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kLSTM = LSTMClassifier(k_vocab_size, k_embedding_dim, maxlen)\n",
        "kLSTM.fit_predict(kX_train, kX_test, ky_train, ky_test, epochs=10)"
      ],
      "metadata": {
        "id": "wl6Tuq9kKA2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try multiclass"
      ],
      "metadata": {
        "id": "LEf6go_Q1f2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class M_LSTMTextClassifier:\n",
        "    def __init__(self, num_classes=4, vocab_size=10000, embedding_dim=128, lstm_units=128):\n",
        "        self.num_classes = num_classes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_units = lstm_units\n",
        "        self.model = self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))\n",
        "        model.add(LSTM(self.lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def preprocess_data(self, text_data, target):\n",
        "        tokenizer = Tokenizer(num_words=self.vocab_size)\n",
        "        tokenizer.fit_on_texts(text_data)\n",
        "        sequences = tokenizer.texts_to_sequences(text_data)\n",
        "        word_index = tokenizer.word_index\n",
        "\n",
        "        max_sequence_length = max([len(seq) for seq in sequences])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "        one_hot_target = tf.keras.utils.to_categorical(target, self.num_classes)\n",
        "\n",
        "        return padded_sequences, one_hot_target\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=32, validation_split=0.2):\n",
        "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        return self.model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "ndGL-mai_Xpd"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_X_y(df):\n",
        "  mtext_data = df['text']# .drop('category', axis=1) causes dimension problem when passed to the Sequential gotta find a way to solve it if added nontext cols\n",
        "  mtarget = df['category']\n",
        "  return mtext_data, mtarget\n",
        "\n",
        "def get_encoded_lstm(mtarget):\n",
        "  mlabel_encoder = LabelEncoder()\n",
        "  y = mlabel_encoder.fit_transform(mtarget)\n",
        "  return mlabel_encoder, y\n",
        "\n",
        "\n",
        "def split_lstm(params, target):\n",
        "  return train_test_split(params, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0L7JB9g3Vg0G"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(X_train, y_train, classnum, epochs = 10):\n",
        "  classifier = M_LSTMTextClassifier(num_classes=classnum, vocab_size=10000)\n",
        "  # Preprocess and train the model\n",
        "  X_train_processed, y_train_processed = classifier.preprocess_data(X_train, y_train)\n",
        "  classifier.train(X_train_processed, y_train_processed, epochs=epochs)\n",
        "  return classifier\n",
        "\n",
        "\n",
        "def test_lstm(classifier, label_encoder, mX_test, my_test):\n",
        "  X_test_processed, y_test_processed = classifier.preprocess_data(mX_test, my_test)\n",
        "  loss, accuracy = classifier.evaluate(X_test_processed, y_test_processed)\n",
        "  print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  # Convert predicted labels to original class names\n",
        "  y_pred = classifier.model.predict(X_test_processed)\n",
        "  predicted_labels = np.argmax(y_pred, axis=1)\n",
        "  predicted_class_names = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "  # Generate classification report\n",
        "  classification_rep = classification_report(y_test_processed.argmax(axis=1), predicted_labels, target_names=label_encoder.classes_)\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_rep)\n",
        "  return\n"
      ],
      "metadata": {
        "id": "p381taOdBG7P"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mX, my = lstm_X_y(text_data)\n",
        "multi_encoder, my_labels = get_encoded_lstm(my)\n",
        "mX_train, mX_test, my_train, my_test = split_lstm(mX, my_labels)\n",
        "\n",
        "mclassifier = train_lstm(mX_train, my_train, len(multi_encoder.classes_), 10)\n",
        "\n",
        "test_lstm(mclassifier, multi_encoder, mX_test, my_test)"
      ],
      "metadata": {
        "id": "ctG78p6njEz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c00142c-a9b0-47b7-c1c1-74518b612f48"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 26s 2s/step - loss: 1.3580 - accuracy: 0.3779 - val_loss: 1.3336 - val_accuracy: 0.3247\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 1.1648 - accuracy: 0.4788 - val_loss: 1.2021 - val_accuracy: 0.3636\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.9591 - accuracy: 0.5472 - val_loss: 1.1423 - val_accuracy: 0.4545\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.8248 - accuracy: 0.5993 - val_loss: 1.0557 - val_accuracy: 0.5065\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.6155 - accuracy: 0.8502 - val_loss: 0.9251 - val_accuracy: 0.7013\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.3584 - accuracy: 0.9381 - val_loss: 1.0392 - val_accuracy: 0.6753\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.5111 - accuracy: 0.7948 - val_loss: 0.9049 - val_accuracy: 0.6104\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.2105 - accuracy: 0.9511 - val_loss: 0.9143 - val_accuracy: 0.6364\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.1272 - accuracy: 0.9870 - val_loss: 0.8399 - val_accuracy: 0.7143\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.0747 - accuracy: 0.9935 - val_loss: 0.8108 - val_accuracy: 0.7143\n",
            "3/3 [==============================] - 1s 113ms/step - loss: 1.3704 - accuracy: 0.4688\n",
            "Test Loss: 1.3704, Test Accuracy: 0.4688\n",
            "3/3 [==============================] - 1s 108ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.40      0.34      0.37        29\n",
            "     message       0.50      0.39      0.44        18\n",
            "     project       0.47      0.61      0.53        23\n",
            "     vacancy       0.52      0.54      0.53        26\n",
            "\n",
            "    accuracy                           0.47        96\n",
            "   macro avg       0.47      0.47      0.47        96\n",
            "weighted avg       0.47      0.47      0.46        96\n",
            "\n"
          ]
        }
      ]
    }
  ]
}