{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Text_classification/JobsMessageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ],
      "metadata": {
        "id": "wlZOb4Dl-Dhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk\n",
        "!pip install emoji --upgrade\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "alxSWiApUsUd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iF4iz0A48-vN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "import re\n",
        "import emoji\n",
        "import string\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from pymystem3 import Mystem\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrXVzNn0UzEC",
        "outputId": "dcae232a-9803-4e43-91cf-eaed5eab2e38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze seeds"
      ],
      "metadata": {
        "id": "Adr7nZLtTrH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "SeqZPHCETtge"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the dataset"
      ],
      "metadata": {
        "id": "5Bc_Iah-_hm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = pd.read_excel('msg_type.xlsx')"
      ],
      "metadata": {
        "id": "GcSsofsF-dMV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "orbCX_lh-Wj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()"
      ],
      "metadata": {
        "id": "PCpZOGIf-Wy0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "Qz286hbW-pdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"russian\")"
      ],
      "metadata": {
        "id": "Eau245xn_Q4N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmaization"
      ],
      "metadata": {
        "id": "gQ-tCd_W_r8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mystem = Mystem()"
      ],
      "metadata": {
        "id": "rAdp3V8i_tpg",
        "outputId": "f5734e3d-3945-462c-f648-f6448197d38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize using TFIDF"
      ],
      "metadata": {
        "id": "dXTCd6Lb_3DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('russian'))"
      ],
      "metadata": {
        "id": "9bS02FX5_5F0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset to parameters and encode target labels"
      ],
      "metadata": {
        "id": "nyQPCc1bC7Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = text_data.copy()\n",
        "#df['category'] = df['category'].replace({'ads': 'message', 'project': 'vacancy'})"
      ],
      "metadata": {
        "id": "9VEDUqDtC-jq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category']"
      ],
      "metadata": {
        "id": "Y6hVYWNpv979",
        "outputId": "8829291a-d428-4331-eccf-3c8aefba0e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          ads\n",
              "1          ads\n",
              "2          ads\n",
              "3          ads\n",
              "4          ads\n",
              "        ...   \n",
              "475    vacancy\n",
              "476    vacancy\n",
              "477    vacancy\n",
              "478    vacancy\n",
              "479    vacancy\n",
              "Name: category, Length: 480, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Label encode categories"
      ],
      "metadata": {
        "id": "PdmxZUdeVnlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_target = label_encoder.fit_transform(df['category'])"
      ],
      "metadata": {
        "id": "hzhggYegVrIY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into params and target values"
      ],
      "metadata": {
        "id": "wRqIDsNJV4Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = encoded_target"
      ],
      "metadata": {
        "id": "e0-QiBASVlxW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform transformation on df"
      ],
      "metadata": {
        "id": "egQ65yKOCyux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text: str) -> str:\n",
        "    return emoji.replace_emoji(text, \" \")\n",
        "\n",
        "\n",
        "def remove_links(text: str) -> str:\n",
        "    return re.sub(r\"http\\S+\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "def remove_usernames_and_emails(text: str) -> str:\n",
        "    \"\"\"Удалеяет юзернеймы и email\"\"\"\n",
        "    return re.sub(r\"\\S*@\\S*\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    \"\"\"Удаляем символы пунктуации\"\"\"\n",
        "    return \"\".join([ch if ch not in string.punctuation else \" \" for ch in text])\n",
        "\n",
        "\n",
        "def remove_numbers(text: str) -> str:\n",
        "    \"\"\"Удаляем числа\"\"\"\n",
        "    return \"\".join([i if not i.isdigit() else \" \" for i in text])\n",
        "\n",
        "\n",
        "def remove_multiple_spaces(text: str) -> str:\n",
        "    \"\"\"Удаляем двойные (и более) пробелы\"\"\"\n",
        "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)"
      ],
      "metadata": {
        "id": "kvAuLGg2DeLL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prep_text(text: str) -> str:\n",
        "  return remove_multiple_spaces(\n",
        "      remove_numbers(\n",
        "          remove_punctuation(\n",
        "              remove_usernames_and_emails(\n",
        "                  remove_links(\n",
        "                      remove_emoji(text)\n",
        "                      )\n",
        "                  )\n",
        "              )\n",
        "          )\n",
        "      )"
      ],
      "metadata": {
        "id": "Ze1Dd5irEFH8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBR1DSgUWjqw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#our new dataset with stemmed lemmatized and later vectorized texts\n",
        "stemmed_lemma_txts = []\n",
        "\n",
        "for text in X:\n",
        "  tok = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "  stem_tok = [stemmer.stem(token) for token in tok]\n",
        "  # lem_tok = [lem for lem in mystem.lemmatize(\" \".join(stem_tok)) if not lem.isspace()]\n",
        "  # stemmed_lemma_txts.append(' '.join(lem_tok))\n",
        "  stemmed_lemma_txts.append(' '.join(stem_tok))\n",
        "\n",
        "df['text_lemm'] = stemmed_lemma_txts"
      ],
      "metadata": {
        "id": "_AoklUd1CyQu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF Vectorize"
      ],
      "metadata": {
        "id": "ao8AJx7yD188"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfd = tfidf_vectorizer.fit_transform(stemmed_lemma_txts)"
      ],
      "metadata": {
        "id": "TPetowxrD0BS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset"
      ],
      "metadata": {
        "id": "EPEBSiRxAGHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pass tfidf'd and transfromed data instead of texts as X"
      ],
      "metadata": {
        "id": "pS8tzS8AVK66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidfd, y, stratify=y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "3FHZ9SUyAHx-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7HX96oveEee1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and train baseline model"
      ],
      "metadata": {
        "id": "ZshtoMLyAgcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C=0.004)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JzH_z-mPADkt",
        "outputId": "3d82be06-89af-4faf-926f-f4e96e295c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.004)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.004)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.004)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "Sy2G_blXEcE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-gapxEVgAfrY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "n1Dd5275EjjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "bS6NmFkhEvw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ],
      "metadata": {
        "id": "UGsDgjo8El3O",
        "outputId": "8a0bbd60-37f8-45d3-ee78-11bb2c7d3d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report"
      ],
      "metadata": {
        "id": "bC_2nOceExNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "G3_u5H3RE2ne",
        "outputId": "6b00f693-bf42-4ab5-8171-933d99745574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.88      0.73      0.80        30\n",
            "           3       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.85      0.85      0.85       120\n",
            "weighted avg       0.85      0.85      0.85       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sum it up"
      ],
      "metadata": {
        "id": "QJXTDgm66jBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Catboost"
      ],
      "metadata": {
        "id": "l75Q3u20xSLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_model = CatBoostClassifier(iterations=400, depth=6, learning_rate=0.04, loss_function='MultiClass', verbose=False)\n",
        "cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRD3V3L5puaX",
        "outputId": "3d1d178c-7108-49ce-9ff7-abb4ff1b6bf9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7b8bacbe3a90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cat_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "class_names = ['ads','message', 'project', 'vacancy']\n",
        "report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB79wrrDsBmo",
        "outputId": "280f2ae1-f382-4817-a206-b74e6247219e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.82\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.91      0.67      0.77        30\n",
            "     message       0.74      0.97      0.84        30\n",
            "     project       0.81      0.73      0.77        30\n",
            "     vacancy       0.84      0.90      0.87        30\n",
            "\n",
            "    accuracy                           0.82       120\n",
            "   macro avg       0.83      0.82      0.81       120\n",
            "weighted avg       0.83      0.82      0.81       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model):\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print('Accuracy:', acc)\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Y_7ouuCQ6iXJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_model = LogisticRegression(C=0.004)\n",
        "nb_model = MultinomialNB()\n",
        "svm_model = SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "                              ('rf', rf_model),\n",
        "                              ('svm', svm_model),\n",
        "                              ('xgb', xgb_model)],\n",
        "                            voting='soft')        # soft for probability-based voting\n",
        "\n",
        "rfensemble = VotingClassifier(estimators=[\n",
        "                              ('rf', rf_model),\n",
        "                              ('svm', svm_model),\n",
        "                              ('rf1', RandomForestClassifier(n_estimators=100, random_state=42))],\n",
        "                            voting='hard')\n",
        "\n",
        "models = [log_model, nb_model, svm_model, rf_model, xgb_model, knn_model, ensemble,rfensemble]\n",
        "model_names = ['logreg', 'bayes', 'SVM', 'RandomForest', 'XGB', 'KNN', 'Ensemble', 'RF_ensemble']"
      ],
      "metadata": {
        "id": "AC9Hns0G68Qs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  print(model_names[i])\n",
        "  print()\n",
        "  test_model(models[i])\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "id": "TYOo_QZ98p01",
        "outputId": "554690b6-dc65-4cf1-e283-8a83515ef064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logreg\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.88      0.73      0.80        30\n",
            "           3       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.85      0.85      0.85       120\n",
            "weighted avg       0.85      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bayes\n",
            "\n",
            "Accuracy: 0.775\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.90      0.79        30\n",
            "           1       0.89      0.53      0.67        30\n",
            "           2       0.70      0.70      0.70        30\n",
            "           3       0.85      0.97      0.91        30\n",
            "\n",
            "    accuracy                           0.78       120\n",
            "   macro avg       0.79      0.78      0.77       120\n",
            "weighted avg       0.79      0.78      0.77       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SVM\n",
            "\n",
            "Accuracy: 0.8583333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81        30\n",
            "           1       0.78      0.93      0.85        30\n",
            "           2       0.89      0.80      0.84        30\n",
            "           3       0.96      0.90      0.93        30\n",
            "\n",
            "    accuracy                           0.86       120\n",
            "   macro avg       0.86      0.86      0.86       120\n",
            "weighted avg       0.86      0.86      0.86       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RandomForest\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.81        30\n",
            "           1       0.74      0.97      0.84        30\n",
            "           2       0.83      0.83      0.83        30\n",
            "           3       0.93      0.90      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.87      0.85      0.85       120\n",
            "weighted avg       0.87      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "XGB\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.73      0.81        30\n",
            "           1       0.78      0.97      0.87        30\n",
            "           2       0.88      0.77      0.82        30\n",
            "           3       0.85      0.93      0.89        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.86      0.85      0.85       120\n",
            "weighted avg       0.86      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "KNN\n",
            "\n",
            "Accuracy: 0.7333333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.77      0.69        30\n",
            "           1       0.95      0.70      0.81        30\n",
            "           2       0.68      0.70      0.69        30\n",
            "           3       0.77      0.77      0.77        30\n",
            "\n",
            "    accuracy                           0.73       120\n",
            "   macro avg       0.76      0.73      0.74       120\n",
            "weighted avg       0.76      0.73      0.74       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ensemble\n",
            "\n",
            "Accuracy: 0.8833333333333333\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.73      0.83        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.90      0.87      0.88        30\n",
            "           3       0.91      0.97      0.94        30\n",
            "\n",
            "    accuracy                           0.88       120\n",
            "   macro avg       0.89      0.88      0.88       120\n",
            "weighted avg       0.89      0.88      0.88       120\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RF_ensemble\n",
            "\n",
            "Accuracy: 0.85\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.70      0.81        30\n",
            "           1       0.74      0.97      0.84        30\n",
            "           2       0.83      0.83      0.83        30\n",
            "           3       0.93      0.90      0.92        30\n",
            "\n",
            "    accuracy                           0.85       120\n",
            "   macro avg       0.87      0.85      0.85       120\n",
            "weighted avg       0.87      0.85      0.85       120\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.inverse_transform([0, 1])"
      ],
      "metadata": {
        "id": "FXRPVw4xjZSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "kECINKJMZ4Y5",
        "outputId": "62fbd3a5-402d-4e5a-df5f-6d47bd7b05e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ищу экспертов, кто хочет продавать свои услуги быстро и легко 🔥\\n\\nПривет, я Олеся - графический и веб-дизайнер экспертов. Я создаю лаконичные, с правильной структурой и собственным вайбом сайты для экспертов на Taplink. \\n\\nВся нужная и важная информация собрана в одном месте, чтобы ваш клиент проходил минимальный путь к покупке 💸\\n\\nЧтобы узнать подробнее пиши в лс \"СОТРУДНИЧЕСТВО\"🤍'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Element"
      ],
      "metadata": {
        "id": "2ViCtPXXZXzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[50]"
      ],
      "metadata": {
        "id": "6sDQJPRAaC7H",
        "outputId": "6ec999ab-e275-4d7f-d25c-3419228aaadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Ищешь копирайтера и PR специалиста? \\n\\nПривет...\n",
              "category                                                   ads\n",
              "text_lemm    ищеш копирайтер и pr специалист привет мен зов...\n",
              "Name: 50, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mywords = []\n",
        "text = df['text'][50]#'Продам гараж. Мопед не мой. Я просто разместил объяву.'\n",
        "tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "stk = [stemmer.stem(token) for token in tk]\n",
        "lmtk = [lem for lem in mystem.lemmatize(\" \".join(stk)) if not lem.isspace()]\n",
        "mywords.append(' '.join(lmtk))\n",
        "wrd = tfidf_vectorizer.transform(mywords)\n",
        "rf_model.predict(wrd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1vw2EZ1WsaZ",
        "outputId": "9f1df401-915e-4f7a-902a-8cf788a18cb0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add Stacking of models"
      ],
      "metadata": {
        "id": "qQe_zR4VkwO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "    ('svm', SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True)),\n",
        "     ('logreg', LogisticRegression(C=0.0045)),\n",
        "    ('xgb', xgb.XGBClassifier())]\n",
        "final_model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42)\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=final_model)\n",
        "test_model(stacking_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANjUw6m3hK1Q",
        "outputId": "b1fca50b-4835-4e40-d5a8-a9f3c476a7a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.875\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        30\n",
            "           1       0.85      0.93      0.89        30\n",
            "           2       0.86      0.80      0.83        30\n",
            "           3       0.90      0.93      0.92        30\n",
            "\n",
            "    accuracy                           0.88       120\n",
            "   macro avg       0.88      0.88      0.87       120\n",
            "weighted avg       0.88      0.88      0.87       120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement as pipeline"
      ],
      "metadata": {
        "id": "kSmrpEmZidDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iPipe:\n",
        "  def __init__(self, estimator):\n",
        "    self.estimator = estimator\n",
        "    self.model = None\n",
        "\n",
        "  def prepipe(self, text):\n",
        "    tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "    stk = [stemmer.stem(token) for token in tk]\n",
        "    return ' '.join(stk)\n",
        "\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    self.model = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(preprocessor=self.prepipe)),\n",
        "        ('clf', self.estimator)])\n",
        "\n",
        "    self.model.fit(X_train, y_train)\n",
        "    return self.model\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.model.predict(x)\n",
        "\n",
        "\n",
        "  def evaluate(self, X_test, y_test):\n",
        "    y_pred = self.predict(X_test)\n",
        "    print('predicted')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "  def fit_predict(self, df):\n",
        "    X = df['text']\n",
        "    y = df['category']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
        "    self.train(X_train, y_train)\n",
        "    print('trained')\n",
        "    self.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "khJv0PCXe7hi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = iPipe(RandomForestClassifier(n_estimators=100, random_state=42, max_depth=6))\n",
        "pipe.fit_predict(text_data)"
      ],
      "metadata": {
        "id": "112mOgwohPuE",
        "outputId": "7272ae77-dc62-4ecd-8338-fbb79532ba9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trained\n",
            "predicted\n",
            "Accuracy: 0.84375\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ads       0.94      0.71      0.81        24\n",
            "     message       0.75      1.00      0.86        24\n",
            "     project       0.89      0.71      0.79        24\n",
            "     vacancy       0.85      0.96      0.90        24\n",
            "\n",
            "    accuracy                           0.84        96\n",
            "   macro avg       0.86      0.84      0.84        96\n",
            "weighted avg       0.86      0.84      0.84        96\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrd = ['Для мегастартапа новый гугл ищу датасатаниста. \\\n",
        "        плачу 300000 баксов\\наносек. надо все и сразу - подготовка, обработка, написание моделей, \\\n",
        "        создание красивеньких графиков и продажа данных. Нужен мидл с запросами джуна и компетенциями сеньора. \\\n",
        "        тимлид и техлид обязанности обязательно. возможно придется искать клиентов, презентовать им работу и вести \\\n",
        "        документации предприятия. Иногда мыть полы. Не задаваться вопросом зачем я нужен в проекте.']\n",
        "pipe.predict(wrd)"
      ],
      "metadata": {
        "id": "AjHf5Hnxiihw",
        "outputId": "4888f64a-21d9-4a41-fc62-468daf6681c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['project'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Word2Vec"
      ],
      "metadata": {
        "id": "hXKBmxgNoKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "T9jkxooNoMMO"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "metadata": {
        "id": "aQzkmraBoOZG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#api.info()\n",
        "word2vec_model = api.load('word2vec-ruscorpora-300')"
      ],
      "metadata": {
        "id": "acgU98cMosMb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "looking = df['text'][0].split()[0]"
      ],
      "metadata": {
        "id": "5FPXBrhdz_Os"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print(word2vec_model.key_to_index[looking])\n",
        "except:\n",
        "  print('freaking rus corpora')"
      ],
      "metadata": {
        "id": "q73IZuOjz7eV",
        "outputId": "9b1e7452-d1d4-4566-e40f-4f26f1e694f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freaking rus corpora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.key_to_index['искать_VERB']"
      ],
      "metadata": {
        "id": "VTjI5RBWzBD9",
        "outputId": "d5623684-2b30-4ba4-bd9c-53b4c8557e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "368"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.key_to_index"
      ],
      "metadata": {
        "id": "ySWwgf07z5pY",
        "outputId": "bd6155fc-5cb4-490f-b476-cff222928746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'весь_DET': 0,\n",
              " 'человек_NOUN': 1,\n",
              " 'мочь_VERB': 2,\n",
              " 'год_NOUN': 3,\n",
              " 'сказать_VERB': 4,\n",
              " 'время_NOUN': 5,\n",
              " 'говорить_VERB': 6,\n",
              " 'становиться_VERB': 7,\n",
              " 'знать_VERB': 8,\n",
              " 'самый_DET': 9,\n",
              " 'дело_NOUN': 10,\n",
              " 'день_NOUN': 11,\n",
              " 'жизнь_NOUN': 12,\n",
              " 'рука_NOUN': 13,\n",
              " 'очень_ADV': 14,\n",
              " 'первый_ADJ': 15,\n",
              " 'давать_VERB': 16,\n",
              " 'новый_ADJ': 17,\n",
              " 'слово_NOUN': 18,\n",
              " 'иметь_VERB': 19,\n",
              " 'большой_ADJ': 20,\n",
              " 'идти_VERB': 21,\n",
              " 'глаз_NOUN': 22,\n",
              " 'место_NOUN': 23,\n",
              " 'лицо_NOUN': 24,\n",
              " 'видеть_VERB': 25,\n",
              " 'хотеть_VERB': 26,\n",
              " 'понимать_VERB': 27,\n",
              " 'должный_ADJ': 28,\n",
              " 'работа_NOUN': 29,\n",
              " 'каждый_DET': 30,\n",
              " 'друг_NOUN': 31,\n",
              " 'голова_NOUN': 32,\n",
              " 'дом_NOUN': 33,\n",
              " 'оставаться_VERB': 34,\n",
              " 'сторона_NOUN': 35,\n",
              " 'начинать_VERB': 36,\n",
              " 'думать_VERB': 37,\n",
              " 'хорошо_ADV': 38,\n",
              " 'жить_VERB': 39,\n",
              " 'стоять_VERB': 40,\n",
              " 'спрашивать_VERB': 41,\n",
              " 'сделать_VERB': 42,\n",
              " 'выходить_VERB': 43,\n",
              " 'последний_ADJ': 44,\n",
              " 'русский_ADJ': 45,\n",
              " 'сила_NOUN': 46,\n",
              " 'получать_VERB': 47,\n",
              " 'какой-то_DET': 48,\n",
              " 'хороший_ADJ': 49,\n",
              " 'случай_NOUN': 50,\n",
              " 'вопрос_NOUN': 51,\n",
              " 'отвечать_VERB': 52,\n",
              " 'россия_NOUN': 53,\n",
              " 'несколько_NUM': 54,\n",
              " 'мир_NOUN': 55,\n",
              " 'город_NOUN': 56,\n",
              " 'делать_VERB': 57,\n",
              " 'пойти_VERB': 58,\n",
              " 'приходить_VERB': 59,\n",
              " 'земля_NOUN': 60,\n",
              " 'принимать_VERB': 61,\n",
              " 'часть_NOUN': 62,\n",
              " 'вид_NOUN': 63,\n",
              " 'ребенок_NOUN': 64,\n",
              " 'конец_NOUN': 65,\n",
              " 'взять_VERB': 66,\n",
              " 'всякий_DET': 67,\n",
              " 'находить_VERB': 68,\n",
              " 'высокий_ADJ': 69,\n",
              " 'сидеть_VERB': 70,\n",
              " 'что-то_PRON': 71,\n",
              " 'отец_NOUN': 72,\n",
              " 'оказываться_VERB': 73,\n",
              " 'вода_NOUN': 74,\n",
              " 'смотреть_VERB': 75,\n",
              " 'москва_NOUN': 76,\n",
              " 'час_NOUN': 77,\n",
              " 'являться_VERB': 78,\n",
              " 'народ_NOUN': 79,\n",
              " 'любить_VERB': 80,\n",
              " 'проходить_VERB': 81,\n",
              " 'никто_PRON': 82,\n",
              " 'образ_NOUN': 83,\n",
              " 'казаться_VERB': 84,\n",
              " 'женщина_NOUN': 85,\n",
              " 'бог_NOUN': 86,\n",
              " 'нога_NOUN': 87,\n",
              " 'страна_NOUN': 88,\n",
              " 'голос_NOUN': 89,\n",
              " 'писать_VERB': 90,\n",
              " 'ночь_NOUN': 91,\n",
              " 'душа_NOUN': 92,\n",
              " 'старый_ADJ': 93,\n",
              " 'лишь_PART': 94,\n",
              " 'также_ADV': 95,\n",
              " 'общий_ADJ': 96,\n",
              " 'называть_VERB': 97,\n",
              " 'молодой_ADJ': 98,\n",
              " 'отношение_NOUN': 99,\n",
              " 'великий_ADJ': 100,\n",
              " 'уходить_VERB': 101,\n",
              " 'деньги_NOUN': 102,\n",
              " 'второй_ADJ': 103,\n",
              " 'мысль_NOUN': 104,\n",
              " 'приходиться_VERB': 105,\n",
              " 'работать_VERB': 106,\n",
              " 'именно_PART': 107,\n",
              " 'оно_PRON': 108,\n",
              " 'дорога_NOUN': 109,\n",
              " 'рассказывать_VERB': 110,\n",
              " 'увидеть_VERB': 111,\n",
              " 'дверь_NOUN': 112,\n",
              " 'вместе_ADV': 113,\n",
              " 'представлять_VERB': 114,\n",
              " 'просто_PART': 115,\n",
              " 'война_NOUN': 116,\n",
              " 'находиться_VERB': 117,\n",
              " 'происходить_VERB': 118,\n",
              " 'книга_NOUN': 119,\n",
              " 'некоторый_DET': 120,\n",
              " 'свет_NOUN': 121,\n",
              " 'минута_NOUN': 122,\n",
              " 'полный_ADJ': 123,\n",
              " 'скоро_ADV': 124,\n",
              " 'вечер_NOUN': 125,\n",
              " 'письмо_NOUN': 126,\n",
              " 'главный_ADJ': 127,\n",
              " 'показывать_VERB': 128,\n",
              " 'стол_NOUN': 129,\n",
              " 'жена_NOUN': 130,\n",
              " 'далеко_ADV': 131,\n",
              " 'мать_NOUN': 132,\n",
              " 'никакой_DET': 133,\n",
              " 'решать_VERB': 134,\n",
              " 'путь_NOUN': 135,\n",
              " 'входить_VERB': 136,\n",
              " 'комната_NOUN': 137,\n",
              " 'имя_NOUN': 138,\n",
              " 'считать_VERB': 139,\n",
              " 'история_NOUN': 140,\n",
              " 'белый_ADJ': 141,\n",
              " 'замечать_VERB': 142,\n",
              " 'сегодня_ADV': 143,\n",
              " 'совершенно_ADV': 144,\n",
              " 'маленький_ADJ': 145,\n",
              " 'нужно_ADV': 146,\n",
              " 'пора_NOUN': 147,\n",
              " 'власть_NOUN': 148,\n",
              " 'сердце_NOUN': 149,\n",
              " 'часто_ADV': 150,\n",
              " 'черный_ADJ': 151,\n",
              " 'узнавать_VERB': 152,\n",
              " 'число_NOUN': 153,\n",
              " 'сын_NOUN': 154,\n",
              " 'право_NOUN': 155,\n",
              " 'кроме_ADP': 156,\n",
              " 'сразу_ADV': 157,\n",
              " 'разный_ADJ': 158,\n",
              " 'система_NOUN': 159,\n",
              " 'против_ADP': 160,\n",
              " 'утро_NOUN': 161,\n",
              " 'лежать_VERB': 162,\n",
              " 'оставлять_VERB': 163,\n",
              " 'общество_NOUN': 164,\n",
              " 'любовь_NOUN': 165,\n",
              " 'вообще_ADV': 166,\n",
              " 'улица_NOUN': 167,\n",
              " 'приезжать_VERB': 168,\n",
              " 'начало_NOUN': 169,\n",
              " 'мало_ADV': 170,\n",
              " 'настоящий_ADJ': 171,\n",
              " 'смерть_NOUN': 172,\n",
              " 'особенно_ADV': 173,\n",
              " 'тысяча_NOUN': 174,\n",
              " 'снова_ADV': 175,\n",
              " 'положение_NOUN': 176,\n",
              " 'продолжать_VERB': 177,\n",
              " 'написать_VERB': 178,\n",
              " 'язык_NOUN': 179,\n",
              " 'читать_VERB': 180,\n",
              " 'советский_ADJ': 181,\n",
              " 'многий_DET': 182,\n",
              " 'возможность_NOUN': 183,\n",
              " 'закон_NOUN': 184,\n",
              " 'ходить_VERB': 185,\n",
              " 'подходить_VERB': 186,\n",
              " 'князь_NOUN': 187,\n",
              " 'машина_NOUN': 188,\n",
              " 'ко_ADP': 189,\n",
              " 'иной_DET': 190,\n",
              " 'среди_ADP': 191,\n",
              " 'появляться_VERB': 192,\n",
              " 'ждать_VERB': 193,\n",
              " 'оба_NUM': 194,\n",
              " 'целый_ADJ': 195,\n",
              " 'бывать_VERB': 196,\n",
              " 'окно_NOUN': 197,\n",
              " 'российский_ADJ': 198,\n",
              " 'чувство_NOUN': 199,\n",
              " 'поэтому_ADV': 200,\n",
              " 'мера_NOUN': 201,\n",
              " 'например_ADV': 202,\n",
              " 'брат_NOUN': 203,\n",
              " 'известный_ADJ': 204,\n",
              " 'помнить_VERB': 205,\n",
              " 'форма_NOUN': 206,\n",
              " 'долго_ADV': 207,\n",
              " 'красный_ADJ': 208,\n",
              " 'просить_VERB': 209,\n",
              " 'месяц_NOUN': 210,\n",
              " 'нужный_ADJ': 211,\n",
              " 'быстро_ADV': 212,\n",
              " 'живой_ADJ': 213,\n",
              " 'приводить_VERB': 214,\n",
              " 'садиться_VERB': 215,\n",
              " 'взгляд_NOUN': 216,\n",
              " 'действие_NOUN': 217,\n",
              " 'условие_NOUN': 218,\n",
              " 'военный_ADJ': 219,\n",
              " 'труд_NOUN': 220,\n",
              " 'собственный_ADJ': 221,\n",
              " 'следовать_VERB': 222,\n",
              " 'открывать_VERB': 223,\n",
              " 'товарищ_NOUN': 224,\n",
              " 'около_ADP': 225,\n",
              " 'начинаться_VERB': 226,\n",
              " 'затем_ADV': 227,\n",
              " 'цель_NOUN': 228,\n",
              " 'движение_NOUN': 229,\n",
              " 'разговор_NOUN': 230,\n",
              " 'внимание_NOUN': 231,\n",
              " 'государственный_ADJ': 232,\n",
              " 'подумать_VERB': 233,\n",
              " 'забывать_VERB': 234,\n",
              " 'чувствовать_VERB': 235,\n",
              " 'предлагать_VERB': 236,\n",
              " 'тело_NOUN': 237,\n",
              " 'назад_ADV': 238,\n",
              " 'помогать_VERB': 239,\n",
              " 'составлять_VERB': 240,\n",
              " 'стена_NOUN': 241,\n",
              " 'проводить_VERB': 242,\n",
              " 'позволять_VERB': 243,\n",
              " 'играть_VERB': 244,\n",
              " 'пять_NUM': 245,\n",
              " 'огромный_ADJ': 246,\n",
              " 'посмотреть_VERB': 247,\n",
              " 'развитие_NOUN': 248,\n",
              " 'суд_NOUN': 249,\n",
              " 'группа_NOUN': 250,\n",
              " 'состояние_NOUN': 251,\n",
              " 'умирать_VERB': 252,\n",
              " 'следующий_ADJ': 253,\n",
              " 'вести_VERB': 254,\n",
              " 'поставлять_VERB': 255,\n",
              " 'средство_NOUN': 256,\n",
              " 'вернуться_VERB': 257,\n",
              " 'слышать_VERB': 258,\n",
              " 'полагать_VERB': 259,\n",
              " 'создавать_VERB': 260,\n",
              " 'результат_NOUN': 261,\n",
              " 'важный_ADJ': 262,\n",
              " 'давно_ADV': 263,\n",
              " 'третий_ADJ': 264,\n",
              " 'собираться_VERB': 265,\n",
              " 'помощь_NOUN': 266,\n",
              " 'армия_NOUN': 267,\n",
              " 'берег_NOUN': 268,\n",
              " 'вызывать_VERB': 269,\n",
              " 'муж_NOUN': 270,\n",
              " 'наука_NOUN': 271,\n",
              " 'лес_NOUN': 272,\n",
              " 'школа_NOUN': 273,\n",
              " 'подымать_VERB': 274,\n",
              " 'связь_NOUN': 275,\n",
              " 'порядок_NOUN': 276,\n",
              " 'речь_NOUN': 277,\n",
              " 'рано_ADV': 278,\n",
              " 'вещь_NOUN': 279,\n",
              " 'решение_NOUN': 280,\n",
              " 'бояться_VERB': 281,\n",
              " 'церковь_NOUN': 282,\n",
              " 'смысл_NOUN': 283,\n",
              " 'область_NOUN': 284,\n",
              " 'век_NOUN': 285,\n",
              " 'московский_ADJ': 286,\n",
              " 'совет_NOUN': 287,\n",
              " 'действительно_ADV': 288,\n",
              " 'двор_NOUN': 289,\n",
              " 'государство_NOUN': 290,\n",
              " 'сильный_ADJ': 291,\n",
              " 'все-таки_PART': 292,\n",
              " 'ехать_VERB': 293,\n",
              " 'ответ_NOUN': 294,\n",
              " 'глядеть_VERB': 295,\n",
              " 'хотеться_VERB': 296,\n",
              " 'иван_NOUN': 297,\n",
              " 'правительство_NOUN': 298,\n",
              " 'из-за_ADP': 299,\n",
              " 'спать_VERB': 300,\n",
              " 'старик_NOUN': 301,\n",
              " 'театр_NOUN': 302,\n",
              " 'вставать_VERB': 303,\n",
              " 'останавливаться_VERB': 304,\n",
              " 'четыре_NUM': 305,\n",
              " 'заниматься_VERB': 306,\n",
              " 'ряд_NOUN': 307,\n",
              " 'обращаться_VERB': 308,\n",
              " 'слушать_VERB': 309,\n",
              " 'бросать_VERB': 310,\n",
              " 'процесс_NOUN': 311,\n",
              " 'значит_ADV': 312,\n",
              " 'занимать_VERB': 313,\n",
              " 'причина_NOUN': 314,\n",
              " 'проблема_NOUN': 315,\n",
              " 'правда_ADV': 316,\n",
              " 'довольно_ADV': 317,\n",
              " 'добрый_ADJ': 318,\n",
              " 'отдавать_VERB': 319,\n",
              " 'статья_NOUN': 320,\n",
              " 'кто-то_PRON': 321,\n",
              " 'встречать_VERB': 322,\n",
              " 'плечо_NOUN': 323,\n",
              " 'го_NOUN': 324,\n",
              " 'подниматься_VERB': 325,\n",
              " 'дух_NOUN': 326,\n",
              " 'подобный_ADJ': 327,\n",
              " 'рядом_ADV': 328,\n",
              " 'должно_ADV': 329,\n",
              " 'убивать_VERB': 330,\n",
              " 'попадать_VERB': 331,\n",
              " 'воздух_NOUN': 332,\n",
              " 'партия_NOUN': 333,\n",
              " 'море_NOUN': 334,\n",
              " 'слишком_ADV': 335,\n",
              " 'рубль_NOUN': 336,\n",
              " 'простой_ADJ': 337,\n",
              " 'любой_DET': 338,\n",
              " 'семья_NOUN': 339,\n",
              " 'квартира_NOUN': 340,\n",
              " 'легко_ADV': 341,\n",
              " 'готовый_ADJ': 342,\n",
              " 'господин_NOUN': 343,\n",
              " 'роль_NOUN': 344,\n",
              " 'момент_NOUN': 345,\n",
              " 'успевать_VERB': 346,\n",
              " 'деятельность_NOUN': 347,\n",
              " 'политический_ADJ': 348,\n",
              " 'задача_NOUN': 349,\n",
              " 'равный_ADJ': 350,\n",
              " 'человеческий_ADJ': 351,\n",
              " 'шаг_NOUN': 352,\n",
              " 'член_NOUN': 353,\n",
              " 'газета_NOUN': 354,\n",
              " 'организация_NOUN': 355,\n",
              " 'близкий_ADJ': 356,\n",
              " 'солнце_NOUN': 357,\n",
              " 'приносить_VERB': 358,\n",
              " 'качество_NOUN': 359,\n",
              " 'требовать_VERB': 360,\n",
              " 'небо_NOUN': 361,\n",
              " 'александр_NOUN': 362,\n",
              " 'поле_NOUN': 363,\n",
              " 'весьма_ADV': 364,\n",
              " 'как-то_ADV': 365,\n",
              " 'улыбаться_VERB': 366,\n",
              " 'борьба_NOUN': 367,\n",
              " 'искать_VERB': 368,\n",
              " 'идея_NOUN': 369,\n",
              " 'неделя_NOUN': 370,\n",
              " 'существовать_VERB': 371,\n",
              " 'река_NOUN': 372,\n",
              " 'девушка_NOUN': 373,\n",
              " 'удаваться_VERB': 374,\n",
              " 'случаться_VERB': 375,\n",
              " 'снимать_VERB': 376,\n",
              " 'объяснять_VERB': 377,\n",
              " 'род_NOUN': 378,\n",
              " 'особый_ADJ': 379,\n",
              " 'основной_ADJ': 380,\n",
              " 'генерал_NOUN': 381,\n",
              " 'крупный_ADJ': 382,\n",
              " 'брать_VERB': 383,\n",
              " 'тяжелый_ADJ': 384,\n",
              " 'войско_NOUN': 385,\n",
              " 'точно_ADV': 386,\n",
              " 'вполне_ADV': 387,\n",
              " 'компания_NOUN': 388,\n",
              " 'петр_NOUN': 389,\n",
              " 'какой-нибудь_DET': 390,\n",
              " 'небольшой_ADJ': 391,\n",
              " 'мнение_NOUN': 392,\n",
              " 'мама_NOUN': 393,\n",
              " 'солдат_NOUN': 394,\n",
              " 'ум_NOUN': 395,\n",
              " 'стараться_VERB': 396,\n",
              " 'держать_VERB': 397,\n",
              " 'служба_NOUN': 398,\n",
              " 'гора_NOUN': 399,\n",
              " 'вера_NOUN': 400,\n",
              " 'искусство_NOUN': 401,\n",
              " 'характер_NOUN': 402,\n",
              " 'сильно_ADV': 403,\n",
              " 'начальник_NOUN': 404,\n",
              " 'внутренний_ADJ': 405,\n",
              " 'гость_NOUN': 406,\n",
              " 'служить_VERB': 407,\n",
              " 'производить_VERB': 408,\n",
              " 'лошадь_NOUN': 409,\n",
              " 'картина_NOUN': 410,\n",
              " 'посылать_VERB': 411,\n",
              " 'опыт_NOUN': 412,\n",
              " 'интерес_NOUN': 413,\n",
              " 'природа_NOUN': 414,\n",
              " 'автор_NOUN': 415,\n",
              " 'значение_NOUN': 416,\n",
              " 'верить_VERB': 417,\n",
              " 'желать_VERB': 418,\n",
              " 'огонь_NOUN': 419,\n",
              " 'деревня_NOUN': 420,\n",
              " 'ибо_CCONJ': 421,\n",
              " 'правый_ADJ': 422,\n",
              " 'словно_CCONJ': 423,\n",
              " 'офицер_NOUN': 424,\n",
              " 'кровь_NOUN': 425,\n",
              " 'длинный_ADJ': 426,\n",
              " 'план_NOUN': 427,\n",
              " 'палец_NOUN': 428,\n",
              " 'малый_ADJ': 429,\n",
              " 'менее_ADV': 430,\n",
              " 'широкий_ADJ': 431,\n",
              " 'николай_NOUN': 432,\n",
              " 'хозяин_NOUN': 433,\n",
              " 'угол_NOUN': 434,\n",
              " 'пить_VERB': 435,\n",
              " 'страшный_ADJ': 436,\n",
              " 'судьба_NOUN': 437,\n",
              " 'сообщать_VERB': 438,\n",
              " 'пол_NOUN': 439,\n",
              " 'десять_NUM': 440,\n",
              " 'точка_NOUN': 441,\n",
              " 'сколько_ADV': 442,\n",
              " 'прямо_PART': 443,\n",
              " 'возникать_VERB': 444,\n",
              " 'государь_NOUN': 445,\n",
              " 'воля_NOUN': 446,\n",
              " 'кажется_ADV': 447,\n",
              " 'относиться_VERB': 448,\n",
              " 'вспомнить_VERB': 449,\n",
              " 'дерево_NOUN': 450,\n",
              " 'трудно_ADV': 451,\n",
              " 'ход_NOUN': 452,\n",
              " 'мальчик_NOUN': 453,\n",
              " 'показываться_VERB': 454,\n",
              " 'царь_NOUN': 455,\n",
              " 'видно_ADV': 456,\n",
              " 'заставлять_VERB': 457,\n",
              " 'течение_NOUN': 458,\n",
              " 'бежать_VERB': 459,\n",
              " 'центр_NOUN': 460,\n",
              " 'переходить_VERB': 461,\n",
              " 'современный_ADJ': 462,\n",
              " 'материал_NOUN': 463,\n",
              " 'передавать_VERB': 464,\n",
              " 'мужчина_NOUN': 465,\n",
              " 'глава_NOUN': 466,\n",
              " 'туда_ADV': 467,\n",
              " 'петербург_NOUN': 468,\n",
              " 'необходимый_ADJ': 469,\n",
              " 'дочь_NOUN': 470,\n",
              " 'счастие_NOUN': 471,\n",
              " 'количество_NOUN': 472,\n",
              " 'поехать_VERB': 473,\n",
              " 'прекрасный_ADJ': 474,\n",
              " 'пусть_PART': 475,\n",
              " 'народный_ADJ': 476,\n",
              " 'темный_ADJ': 477,\n",
              " 'уезжать_VERB': 478,\n",
              " 'указывать_VERB': 479,\n",
              " 'тихо_ADV': 480,\n",
              " 'край_NOUN': 481,\n",
              " 'ветер_NOUN': 482,\n",
              " 'подавать_VERB': 483,\n",
              " 'уровень_NOUN': 484,\n",
              " 'класс_NOUN': 485,\n",
              " 'правило_NOUN': 486,\n",
              " 'вместо_ADP': 487,\n",
              " 'событие_NOUN': 488,\n",
              " 'уметь_VERB': 489,\n",
              " 'состоять_VERB': 490,\n",
              " 'немного_ADV': 491,\n",
              " 'похожий_ADJ': 492,\n",
              " 'встречаться_VERB': 493,\n",
              " 'едва_ADV': 494,\n",
              " 'единственный_ADJ': 495,\n",
              " 'бывший_ADJ': 496,\n",
              " 'поздно_ADV': 497,\n",
              " 'домой_ADV': 498,\n",
              " 'возвращаться_VERB': 499,\n",
              " 'завод_NOUN': 500,\n",
              " 'направление_NOUN': 501,\n",
              " 'повторять_VERB': 502,\n",
              " 'странный_ADJ': 503,\n",
              " 'общественный_ADJ': 504,\n",
              " 'чистый_ADJ': 505,\n",
              " 'бумага_NOUN': 506,\n",
              " 'плохо_ADV': 507,\n",
              " 'кричать_VERB': 508,\n",
              " 'память_NOUN': 509,\n",
              " 'различный_ADJ': 510,\n",
              " 'сначала_ADV': 511,\n",
              " 'свобода_NOUN': 512,\n",
              " 'андрей_NOUN': 513,\n",
              " 'надежда_NOUN': 514,\n",
              " 'предмет_NOUN': 515,\n",
              " 'производство_NOUN': 516,\n",
              " 'получаться_VERB': 517,\n",
              " 'управление_NOUN': 518,\n",
              " 'несмотря_ADV': 519,\n",
              " 'данный_ADJ': 520,\n",
              " 'цена_NOUN': 521,\n",
              " 'пример_NOUN': 522,\n",
              " 'двадцать_NUM': 523,\n",
              " 'стоить_VERB': 524,\n",
              " 'соглашаться_VERB': 525,\n",
              " 'сюда_ADV': 526,\n",
              " 'определять_VERB': 527,\n",
              " 'счет_NOUN': 528,\n",
              " 'пытаться_VERB': 529,\n",
              " 'собирать_VERB': 530,\n",
              " 'вчера_ADV': 531,\n",
              " 'отдельный_ADJ': 532,\n",
              " 'касаться_VERB': 533,\n",
              " 'вовсе_ADV': 534,\n",
              " 'толпа_NOUN': 535,\n",
              " 'половина_NOUN': 536,\n",
              " 'сон_NOUN': 537,\n",
              " 'союз_NOUN': 538,\n",
              " 'губа_NOUN': 539,\n",
              " 'хлеб_NOUN': 540,\n",
              " 'чужой_ADJ': 541,\n",
              " 'обращать_VERB': 542,\n",
              " 'выступать_VERB': 543,\n",
              " 'стих_NOUN': 544,\n",
              " 'спина_NOUN': 545,\n",
              " 'культура_NOUN': 546,\n",
              " 'революция_NOUN': 547,\n",
              " 'местный_ADJ': 548,\n",
              " 'желание_NOUN': 549,\n",
              " 'встреча_NOUN': 550,\n",
              " 'состав_NOUN': 551,\n",
              " 'что-нибудь_PRON': 552,\n",
              " 'сад_NOUN': 553,\n",
              " 'устраивать_VERB': 554,\n",
              " 'граница_NOUN': 555,\n",
              " 'грудь_NOUN': 556,\n",
              " 'степень_NOUN': 557,\n",
              " 'прочитывать_VERB': 558,\n",
              " 'вперед_ADV': 559,\n",
              " 'министр_NOUN': 560,\n",
              " 'мужик_NOUN': 561,\n",
              " 'писатель_NOUN': 562,\n",
              " 'участие_NOUN': 563,\n",
              " 'глубокий_ADJ': 564,\n",
              " 'программа_NOUN': 565,\n",
              " 'произносить_VERB': 566,\n",
              " 'сестра_NOUN': 567,\n",
              " 'успех_NOUN': 568,\n",
              " 'красивый_ADJ': 569,\n",
              " 'радость_NOUN': 570,\n",
              " 'прежде_ADP': 571,\n",
              " 'камень_NOUN': 572,\n",
              " 'ученый_NOUN': 573,\n",
              " 'девочка_NOUN': 574,\n",
              " 'факт_NOUN': 575,\n",
              " 'рост_NOUN': 576,\n",
              " 'сознание_NOUN': 577,\n",
              " 'прежний_ADJ': 578,\n",
              " 'страх_NOUN': 579,\n",
              " 'институт_NOUN': 580,\n",
              " 'доктор_NOUN': 581,\n",
              " 'сергей_NOUN': 582,\n",
              " 'орган_NOUN': 583,\n",
              " 'масса_NOUN': 584,\n",
              " 'очередь_NOUN': 585,\n",
              " 'дорогой_ADJ': 586,\n",
              " 'достаточно_ADV': 587,\n",
              " 'отказываться_VERB': 588,\n",
              " 'иванович_NOUN': 589,\n",
              " 'чай_NOUN': 590,\n",
              " 'игра_NOUN': 591,\n",
              " 'рассказ_NOUN': 592,\n",
              " 'отправляться_VERB': 593,\n",
              " 'столько_NUM': 594,\n",
              " 'волос_NOUN': 595,\n",
              " 'личный_ADJ': 596,\n",
              " 'поступать_VERB': 597,\n",
              " 'ожидать_VERB': 598,\n",
              " 'однажды_ADV': 599,\n",
              " 'район_NOUN': 600,\n",
              " 'действовать_VERB': 601,\n",
              " 'милый_ADJ': 602,\n",
              " 'зал_NOUN': 603,\n",
              " 'песня_NOUN': 604,\n",
              " 'журнал_NOUN': 605,\n",
              " 'купить_VERB': 606,\n",
              " 'несколько_ADV': 607,\n",
              " 'основа_NOUN': 608,\n",
              " 'герой_NOUN': 609,\n",
              " 'святой_ADJ': 610,\n",
              " 'столь_ADV': 611,\n",
              " 'сцена_NOUN': 612,\n",
              " 'признавать_VERB': 613,\n",
              " 'французский_ADJ': 614,\n",
              " 'предприятие_NOUN': 615,\n",
              " 'больший_ADJ': 616,\n",
              " 'круг_NOUN': 617,\n",
              " 'период_NOUN': 618,\n",
              " 'литература_NOUN': 619,\n",
              " 'нос_NOUN': 620,\n",
              " 'снег_NOUN': 621,\n",
              " 'слеза_NOUN': 622,\n",
              " 'завтра_ADV': 623,\n",
              " 'поэт_NOUN': 624,\n",
              " 'напоминать_VERB': 625,\n",
              " 'смочь_VERB': 626,\n",
              " 'рынок_NOUN': 627,\n",
              " 'враг_NOUN': 628,\n",
              " 'ухо_NOUN': 629,\n",
              " 'правда_NOUN': 630,\n",
              " 'серьезный_ADJ': 631,\n",
              " 'вокруг_ADP': 632,\n",
              " 'немой_ADJ': 633,\n",
              " 'полк_NOUN': 634,\n",
              " 'связывать_VERB': 635,\n",
              " 'основание_NOUN': 636,\n",
              " 'большинство_NOUN': 637,\n",
              " 'способ_NOUN': 638,\n",
              " 'близко_ADV': 639,\n",
              " 'исследование_NOUN': 640,\n",
              " 'собака_NOUN': 641,\n",
              " 'лето_NOUN': 642,\n",
              " 'достигать_VERB': 643,\n",
              " 'проект_NOUN': 644,\n",
              " 'знакомый_ADJ': 645,\n",
              " 'использовать_VERB': 646,\n",
              " 'ясно_ADV': 647,\n",
              " 'дома_ADV': 648,\n",
              " 'представитель_NOUN': 649,\n",
              " 'художник_NOUN': 650,\n",
              " 'доходить_VERB': 651,\n",
              " 'средний_ADJ': 652,\n",
              " 'свободный_ADJ': 653,\n",
              " 'плохой_ADJ': 654,\n",
              " 'болезнь_NOUN': 655,\n",
              " 'линия_NOUN': 656,\n",
              " 'рабочий_ADJ': 657,\n",
              " 'президент_NOUN': 658,\n",
              " 'бой_NOUN': 659,\n",
              " 'научный_ADJ': 660,\n",
              " 'немец_NOUN': 661,\n",
              " 'заявлять_VERB': 662,\n",
              " 'выражение_NOUN': 663,\n",
              " 'крайний_ADJ': 664,\n",
              " 'явление_NOUN': 665,\n",
              " 'врач_NOUN': 666,\n",
              " 'образование_NOUN': 667,\n",
              " 'остальной_DET': 668,\n",
              " 'старший_ADJ': 669,\n",
              " 'повод_NOUN': 670,\n",
              " 'население_NOUN': 671,\n",
              " 'известно_ADV': 672,\n",
              " 'объявлять_VERB': 673,\n",
              " 'золотой_ADJ': 674,\n",
              " 'роман_NOUN': 675,\n",
              " 'впечатление_NOUN': 676,\n",
              " 'тип_NOUN': 677,\n",
              " 'рабочий_NOUN': 678,\n",
              " 'принадлежать_VERB': 679,\n",
              " 'исчезать_VERB': 680,\n",
              " 'владимир_NOUN': 681,\n",
              " 'слава_NOUN': 682,\n",
              " 'влияние_NOUN': 683,\n",
              " 'молчать_VERB': 684,\n",
              " 'собрание_NOUN': 685,\n",
              " 'закрывать_VERB': 686,\n",
              " 'интересный_ADJ': 687,\n",
              " 'срок_NOUN': 688,\n",
              " 'устанавливать_VERB': 689,\n",
              " 'рот_NOUN': 690,\n",
              " 'обстоятельство_NOUN': 691,\n",
              " 'отмечать_VERB': 692,\n",
              " 'кабинет_NOUN': 693,\n",
              " 'возможный_ADJ': 694,\n",
              " 'немецкий_ADJ': 695,\n",
              " 'фронт_NOUN': 696,\n",
              " 'удар_NOUN': 697,\n",
              " 'политика_NOUN': 698,\n",
              " 'переставать_VERB': 699,\n",
              " 'внешний_ADJ': 700,\n",
              " 'ситуация_NOUN': 701,\n",
              " 'номер_NOUN': 702,\n",
              " 'выбирать_VERB': 703,\n",
              " 'либо_CCONJ': 704,\n",
              " 'тема_NOUN': 705,\n",
              " 'граф_NOUN': 706,\n",
              " 'легкий_ADJ': 707,\n",
              " 'петь_VERB': 708,\n",
              " 'божий_ADJ': 709,\n",
              " 'музыка_NOUN': 710,\n",
              " 'откуда_ADV': 711,\n",
              " 'обычный_ADJ': 712,\n",
              " 'услышать_VERB': 713,\n",
              " 'исторический_ADJ': 714,\n",
              " 'директор_NOUN': 715,\n",
              " 'европа_NOUN': 716,\n",
              " 'тонкий_ADJ': 717,\n",
              " 'пользоваться_VERB': 718,\n",
              " 'бросаться_VERB': 719,\n",
              " 'прежде_ADV': 720,\n",
              " 'корабль_NOUN': 721,\n",
              " 'холодный_ADJ': 722,\n",
              " 'надеяться_VERB': 723,\n",
              " 'детский_ADJ': 724,\n",
              " 'знание_NOUN': 725,\n",
              " 'зеленый_ADJ': 726,\n",
              " 'содержание_NOUN': 727,\n",
              " 'бедный_ADJ': 728,\n",
              " 'дядя_NOUN': 729,\n",
              " 'наверное_ADV': 730,\n",
              " 'наиболее_ADV': 731,\n",
              " 'сие_PRON': 732,\n",
              " 'социальный_ADJ': 733,\n",
              " 'размер_NOUN': 734,\n",
              " 'мелкий_ADJ': 735,\n",
              " 'прощать_VERB': 736,\n",
              " 'причем_CCONJ': 737,\n",
              " 'прошлый_ADJ': 738,\n",
              " 'мировой_ADJ': 739,\n",
              " 'лист_NOUN': 740,\n",
              " 'документ_NOUN': 741,\n",
              " 'звать_VERB': 742,\n",
              " 'площадь_NOUN': 743,\n",
              " 'бить_VERB': 744,\n",
              " 'вероятно_ADV': 745,\n",
              " 'командир_NOUN': 746,\n",
              " 'смеяться_VERB': 747,\n",
              " 'понятие_NOUN': 748,\n",
              " 'приказывать_VERB': 749,\n",
              " 'городской_ADJ': 750,\n",
              " 'крестьянин_NOUN': 751,\n",
              " 'существо_NOUN': 752,\n",
              " 'пустой_ADJ': 753,\n",
              " 'шесть_NUM': 754,\n",
              " 'высоко_ADV': 755,\n",
              " 'счастливый_ADJ': 756,\n",
              " 'короткий_ADJ': 757,\n",
              " 'принцип_NOUN': 758,\n",
              " 'железный_ADJ': 759,\n",
              " 'обед_NOUN': 760,\n",
              " 'утверждать_VERB': 761,\n",
              " 'будущее_NOUN': 762,\n",
              " 'звук_NOUN': 763,\n",
              " 'возможно_ADV': 764,\n",
              " 'название_NOUN': 765,\n",
              " 'ставить_VERB': 766,\n",
              " 'зуб_NOUN': 767,\n",
              " 'павел_NOUN': 768,\n",
              " 'представление_NOUN': 769,\n",
              " 'экономический_ADJ': 770,\n",
              " 'удовольствие_NOUN': 771,\n",
              " 'потерять_VERB': 772,\n",
              " 'создание_NOUN': 773,\n",
              " 'национальный_ADJ': 774,\n",
              " 'данные_NOUN': 775,\n",
              " 'цвет_NOUN': 776,\n",
              " 'гораздо_ADV': 777,\n",
              " 'медленно_ADV': 778,\n",
              " 'учитель_NOUN': 779,\n",
              " 'метод_NOUN': 780,\n",
              " 'станция_NOUN': 781,\n",
              " 'капитан_NOUN': 782,\n",
              " 'богатый_ADJ': 783,\n",
              " 'необходимо_ADV': 784,\n",
              " 'требование_NOUN': 785,\n",
              " 'сквозь_ADP': 786,\n",
              " 'родитель_NOUN': 787,\n",
              " 'ребята_NOUN': 788,\n",
              " 'вскоре_ADV': 789,\n",
              " 'левый_ADJ': 790,\n",
              " 'древний_ADJ': 791,\n",
              " 'способный_ADJ': 792,\n",
              " 'носить_VERB': 793,\n",
              " 'миллион_NOUN': 794,\n",
              " 'веселый_ADJ': 795,\n",
              " 'духовный_ADJ': 796,\n",
              " 'низкий_ADJ': 797,\n",
              " 'серый_ADJ': 798,\n",
              " 'прямой_ADJ': 799,\n",
              " 'родной_ADJ': 800,\n",
              " 'выполнять_VERB': 801,\n",
              " 'спокойно_ADV': 802,\n",
              " 'рассматривать_VERB': 803,\n",
              " 'наступать_VERB': 804,\n",
              " 'команда_NOUN': 805,\n",
              " 'глубина_NOUN': 806,\n",
              " 'американский_ADJ': 807,\n",
              " 'недавно_ADV': 808,\n",
              " 'единый_ADJ': 809,\n",
              " 'многие_PRON': 810,\n",
              " 'хозяйство_NOUN': 811,\n",
              " 'определенный_ADJ': 812,\n",
              " 'исполнять_VERB': 813,\n",
              " 'вновь_ADV': 814,\n",
              " 'остров_NOUN': 815,\n",
              " 'ссср_NOUN': 816,\n",
              " 'цветок_NOUN': 817,\n",
              " 'гражданин_NOUN': 818,\n",
              " 'информация_NOUN': 819,\n",
              " 'выход_NOUN': 820,\n",
              " 'знаменитый_ADJ': 821,\n",
              " 'зима_NOUN': 822,\n",
              " 'выпивать_VERB': 823,\n",
              " 'никак_ADV': 824,\n",
              " 'обнаруживать_VERB': 825,\n",
              " 'улыбка_NOUN': 826,\n",
              " 'плакать_VERB': 827,\n",
              " 'решаться_VERB': 828,\n",
              " 'ах_INTJ': 829,\n",
              " 'профессор_NOUN': 830,\n",
              " 'папа_NOUN': 831,\n",
              " 'назначать_VERB': 832,\n",
              " 'изменение_NOUN': 833,\n",
              " 'взглядывать_VERB': 834,\n",
              " 'приглашать_VERB': 835,\n",
              " 'впервые_ADV': 836,\n",
              " 'зрение_NOUN': 837,\n",
              " 'победа_NOUN': 838,\n",
              " 'председатель_NOUN': 839,\n",
              " 'продавать_VERB': 840,\n",
              " 'называться_VERB': 841,\n",
              " 'ради_ADP': 842,\n",
              " 'колено_NOUN': 843,\n",
              " 'расти_VERB': 844,\n",
              " 'элемент_NOUN': 845,\n",
              " 'честь_NOUN': 846,\n",
              " 'доказывать_VERB': 847,\n",
              " 'множество_NOUN': 848,\n",
              " 'почувствовать_VERB': 849,\n",
              " 'пространство_NOUN': 850,\n",
              " 'противник_NOUN': 851,\n",
              " 'кончаться_VERB': 852,\n",
              " 'король_NOUN': 853,\n",
              " 'совершать_VERB': 854,\n",
              " 'необходимость_NOUN': 855,\n",
              " 'английский_ADJ': 856,\n",
              " 'михаил_NOUN': 857,\n",
              " 'зато_CCONJ': 858,\n",
              " 'ложиться_VERB': 859,\n",
              " 'нравиться_VERB': 860,\n",
              " 'польза_NOUN': 861,\n",
              " 'обычно_ADV': 862,\n",
              " 'значительный_ADJ': 863,\n",
              " 'тотчас_ADV': 864,\n",
              " 'комитет_NOUN': 865,\n",
              " 'сложный_ADJ': 866,\n",
              " 'предложение_NOUN': 867,\n",
              " 'анна_NOUN': 868,\n",
              " 'карман_NOUN': 869,\n",
              " 'алексей_NOUN': 870,\n",
              " 'особенность_NOUN': 871,\n",
              " 'сохранять_VERB': 872,\n",
              " 'слабый_ADJ': 873,\n",
              " 'сто_NUM': 874,\n",
              " 'приниматься_VERB': 875,\n",
              " 'пара_NOUN': 876,\n",
              " 'здание_NOUN': 877,\n",
              " 'дама_NOUN': 878,\n",
              " 'упасть_VERB': 879,\n",
              " 'читатель_NOUN': 880,\n",
              " 'кстати_ADV': 881,\n",
              " 'среда_NOUN': 882,\n",
              " 'звезда_NOUN': 883,\n",
              " 'спасать_VERB': 884,\n",
              " 'студент_NOUN': 885,\n",
              " 'василий_NOUN': 886,\n",
              " 'участок_NOUN': 887,\n",
              " 'доставать_VERB': 888,\n",
              " 'оружие_NOUN': 889,\n",
              " 'телефон_NOUN': 890,\n",
              " 'вспоминать_VERB': 891,\n",
              " 'личность_NOUN': 892,\n",
              " 'умный_ADJ': 893,\n",
              " 'черта_NOUN': 894,\n",
              " 'прийти_VERB': 895,\n",
              " 'тихий_ADJ': 896,\n",
              " 'волна_NOUN': 897,\n",
              " 'дальнейший_ADJ': 898,\n",
              " 'поскольку_CCONJ': 899,\n",
              " 'отправлять_VERB': 900,\n",
              " 'долгий_ADJ': 901,\n",
              " 'учиться_VERB': 902,\n",
              " 'постоянно_ADV': 903,\n",
              " 'допускать_VERB': 904,\n",
              " 'ясный_ADJ': 905,\n",
              " 'позиция_NOUN': 906,\n",
              " 'нести_VERB': 907,\n",
              " 'ворота_NOUN': 908,\n",
              " 'предполагать_VERB': 909,\n",
              " 'сколько_CCONJ': 910,\n",
              " 'удивляться_VERB': 911,\n",
              " 'выражать_VERB': 912,\n",
              " 'очевидно_ADV': 913,\n",
              " 'боевой_ADJ': 914,\n",
              " 'сталин_NOUN': 915,\n",
              " 'отсутствие_NOUN': 916,\n",
              " 'верный_ADJ': 917,\n",
              " 'праздник_NOUN': 918,\n",
              " 'фигура_NOUN': 919,\n",
              " 'иметься_VERB': 920,\n",
              " 'парень_NOUN': 921,\n",
              " 'ужас_NOUN': 922,\n",
              " 'специальный_ADJ': 923,\n",
              " 'фильм_NOUN': 924,\n",
              " 'самолет_NOUN': 925,\n",
              " 'поддерживать_VERB': 926,\n",
              " 'предел_NOUN': 927,\n",
              " 'обеспечивать_VERB': 928,\n",
              " 'десяток_NOUN': 929,\n",
              " 'птица_NOUN': 930,\n",
              " 'вино_NOUN': 931,\n",
              " 'баба_NOUN': 932,\n",
              " 'произведение_NOUN': 933,\n",
              " 'комиссия_NOUN': 934,\n",
              " 'житель_NOUN': 935,\n",
              " 'где-то_ADV': 936,\n",
              " 'попросить_VERB': 937,\n",
              " 'будущий_ADJ': 938,\n",
              " 'нынешний_ADJ': 939,\n",
              " 'магазин_NOUN': 940,\n",
              " 'испытывать_VERB': 941,\n",
              " 'вздыхать_VERB': 942,\n",
              " 'сходить_VERB': 943,\n",
              " 'теория_NOUN': 944,\n",
              " 'пускать_VERB': 945,\n",
              " 'погибать_VERB': 946,\n",
              " 'окружать_VERB': 947,\n",
              " 'обещать_VERB': 948,\n",
              " 'просто_ADV': 949,\n",
              " 'высота_NOUN': 950,\n",
              " 'морской_ADJ': 951,\n",
              " 'париж_NOUN': 952,\n",
              " 'прием_NOUN': 953,\n",
              " 'описывать_VERB': 954,\n",
              " 'тень_NOUN': 955,\n",
              " 'видимо_ADV': 956,\n",
              " 'источник_NOUN': 957,\n",
              " 'черт_NOUN': 958,\n",
              " 'выпускать_VERB': 959,\n",
              " 'объект_NOUN': 960,\n",
              " 'сделаться_VERB': 961,\n",
              " 'банк_NOUN': 962,\n",
              " 'возраст_NOUN': 963,\n",
              " 'долг_NOUN': 964,\n",
              " 'поезд_NOUN': 965,\n",
              " 'сведение_NOUN': 966,\n",
              " 'западный_ADJ': 967,\n",
              " 'кой_DET': 968,\n",
              " 'животное_NOUN': 969,\n",
              " 'дума_NOUN': 970,\n",
              " 'выдавать_VERB': 971,\n",
              " 'открытый_ADJ': 972,\n",
              " 'считаться_VERB': 973,\n",
              " 'международный_ADJ': 974,\n",
              " 'горячий_ADJ': 975,\n",
              " 'направлять_VERB': 976,\n",
              " 'главное_ADV': 977,\n",
              " 'красота_NOUN': 978,\n",
              " 'благодаря_ADP': 979,\n",
              " 'существование_NOUN': 980,\n",
              " 'привозить_VERB': 981,\n",
              " 'далее_ADV': 982,\n",
              " 'автомобиль_NOUN': 983,\n",
              " 'светлый_ADJ': 984,\n",
              " 'пожалуй_PART': 985,\n",
              " 'карта_NOUN': 986,\n",
              " 'вступать_VERB': 987,\n",
              " 'защита_NOUN': 988,\n",
              " 'петрович_NOUN': 989,\n",
              " 'фамилия_NOUN': 990,\n",
              " 'больной_NOUN': 991,\n",
              " 'делаться_VERB': 992,\n",
              " 'точно_CCONJ': 993,\n",
              " 'представляться_VERB': 994,\n",
              " 'бабушка_NOUN': 995,\n",
              " 'борис_NOUN': 996,\n",
              " 'весна_NOUN': 997,\n",
              " 'сотня_NOUN': 998,\n",
              " 'построить_VERB': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(word2vec_model)"
      ],
      "metadata": {
        "id": "41y_wAMR0h7T",
        "outputId": "cf9df866-e4e2-48d7-b278-d859531fabe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on KeyedVectors in module gensim.models.keyedvectors object:\n",
            "\n",
            "class KeyedVectors(gensim.utils.SaveLoad)\n",
            " |  KeyedVectors(vector_size, count=0, dtype=<class 'numpy.float32'>, mapfile_path=None)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      KeyedVectors\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, key)\n",
            " |  \n",
            " |  __getitem__(self, key_or_keys)\n",
            " |      Get vector representation of `key_or_keys`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key_or_keys : {str, list of str, int, list of int}\n",
            " |          Requested key or list-of-keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
            " |  \n",
            " |  __init__(self, vector_size, count=0, dtype=<class 'numpy.float32'>, mapfile_path=None)\n",
            " |      Mapping between keys (such as words) and vectors for :class:`~gensim.models.Word2Vec`\n",
            " |      and related models.\n",
            " |      \n",
            " |      Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
            " |      \n",
            " |      To support the needs of specific models and other downstream uses, you can also set\n",
            " |      additional attributes via the :meth:`~gensim.models.keyedvectors.KeyedVectors.set_vecattr`\n",
            " |      and :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vecattr` methods.\n",
            " |      Note that all such attributes under the same `attr` name must have compatible `numpy`\n",
            " |      types, as the type and storage array for such attributes is established by the 1st time such\n",
            " |      `attr` is set.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector_size : int\n",
            " |          Intended number of dimensions for all contained vectors.\n",
            " |      count : int, optional\n",
            " |          If provided, vectors wil be pre-allocated for at least this many vectors. (Otherwise\n",
            " |          they can be added later.)\n",
            " |      dtype : type, optional\n",
            " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
            " |          another type is provided here.\n",
            " |      mapfile_path : string, optional\n",
            " |          Currently unused.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |  \n",
            " |  __setitem__(self, keys, weights)\n",
            " |      Add keys and theirs vectors in a manual way.\n",
            " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
            " |      \n",
            " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
            " |      with `replace=True`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : {str, int, list of (str or int)}\n",
            " |          keys specified by their string or int ids.\n",
            " |      weights: list of numpy.ndarray or numpy.ndarray\n",
            " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  add_vector(self, key, vector)\n",
            " |      Add one new vector at the given key, into existing slot if available.\n",
            " |      \n",
            " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
            " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key: str\n",
            " |          Key identifier of the added vector.\n",
            " |      vector: numpy.ndarray\n",
            " |          1D numpy array with the vector values.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
            " |          ``self.index_to_key[result] == key``.\n",
            " |  \n",
            " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
            " |      Append keys and their vectors in a manual way.\n",
            " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : list of (str or int)\n",
            " |          Keys specified by string or int ids.\n",
            " |      weights: list of numpy.ndarray or numpy.ndarray\n",
            " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
            " |      replace: bool, optional\n",
            " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
            " |          if True - replace vectors, otherwise - keep old vectors.\n",
            " |  \n",
            " |  allocate_vecattrs(self, attrs=None, types=None)\n",
            " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
            " |      \n",
            " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
            " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
            " |      So this allocation targets that size.\n",
            " |  \n",
            " |  closer_than(self, key1, key2)\n",
            " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
            " |  \n",
            " |  distance(self, w1, w2)\n",
            " |      Compute cosine distance between two keys.\n",
            " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      w1 : str\n",
            " |          Input key.\n",
            " |      w2 : str\n",
            " |          Input key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Distance between `w1` and `w2`.\n",
            " |  \n",
            " |  distances(self, word_or_vector, other_words=())\n",
            " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
            " |      If `other_words` is empty, return distance between `word_or_vector` and all words in vocab.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_or_vector : {str, numpy.ndarray}\n",
            " |          Word or vector from which distances are to be computed.\n",
            " |      other_words : iterable of str\n",
            " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
            " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.array\n",
            " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
            " |      \n",
            " |      Raises\n",
            " |      -----\n",
            " |      KeyError\n",
            " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
            " |  \n",
            " |  doesnt_match(self, words)\n",
            " |      Which key from the given list doesn't go with the others?\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      words : list of str\n",
            " |          List of keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          The key further away from the mean of all keys.\n",
            " |  \n",
            " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
            " |      Compute performance of the model on an analogy test set.\n",
            " |      \n",
            " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
            " |      plus there's one aggregate summary at the end.\n",
            " |      \n",
            " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
            " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      analogies : str\n",
            " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
            " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
            " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
            " |          in modern word embedding models).\n",
            " |      case_insensitive : bool, optional\n",
            " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
            " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
            " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
            " |          (also the most frequent if vocabulary is sorted) is taken.\n",
            " |      dummy4unknown : bool, optional\n",
            " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
            " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
            " |      similarity_function : str, optional\n",
            " |          Function name used for similarity calculation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          The overall evaluation score on the entire evaluation set\n",
            " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
            " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
            " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
            " |          keys 'correct' and 'incorrect'.\n",
            " |  \n",
            " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
            " |      Compute correlation of the model with human similarity judgments.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      More datasets can be found at\n",
            " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
            " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      pairs : str\n",
            " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
            " |          See `test/test_data/wordsim353.tsv` as example.\n",
            " |      delimiter : str, optional\n",
            " |          Separator in `pairs` file.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
            " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
            " |          in modern word embedding models).\n",
            " |      case_insensitive : bool, optional\n",
            " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
            " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
            " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
            " |          (also the most frequent if vocabulary is sorted) is taken.\n",
            " |      dummy4unknown : bool, optional\n",
            " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
            " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      pearson : tuple of (float, float)\n",
            " |          Pearson correlation coefficient with 2-tailed p-value.\n",
            " |      spearman : tuple of (float, float)\n",
            " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
            " |          similarities produced by the model itself, with 2-tailed p-value.\n",
            " |      oov_ratio : float\n",
            " |          The ratio of pairs with unknown words.\n",
            " |  \n",
            " |  fill_norms(self, force=False)\n",
            " |      Ensure per-vector norms are available.\n",
            " |      \n",
            " |      Any code which modifies vectors should ensure the accompanying norms are\n",
            " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
            " |  \n",
            " |  get_index(self, key, default=None)\n",
            " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
            " |      backing vectors array.\n",
            " |  \n",
            " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
            " |      Get the mean vector for a given list of keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      keys : list of (str or int or ndarray)\n",
            " |          Keys specified by string or int ids or numpy array.\n",
            " |      weights : list of float or numpy.ndarray, optional\n",
            " |          1D array of same size of `keys` specifying the weight for each key.\n",
            " |      pre_normalize : bool, optional\n",
            " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
            " |          If False, individual keyvector will not be normalized.\n",
            " |      post_normalize: bool, optional\n",
            " |          Flag indicating whether to normalize the final mean vector.\n",
            " |          If True, normalized mean vector will be return.\n",
            " |      ignore_missing : bool, optional\n",
            " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      numpy.ndarray\n",
            " |          Mean vector for the list of keys.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      \n",
            " |      ValueError\n",
            " |          If the size of the list of `keys` and `weights` doesn't match.\n",
            " |      KeyError\n",
            " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
            " |  \n",
            " |  get_normed_vectors(self)\n",
            " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
            " |      \n",
            " |      To see which key corresponds to which vector = which array row, refer\n",
            " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray:\n",
            " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
            " |          along the rows (key vectors).\n",
            " |  \n",
            " |  get_vecattr(self, key, attr)\n",
            " |      Get attribute value associated with given key.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Vector key for which to fetch the attribute value.\n",
            " |      attr : str\n",
            " |          Name of the additional attribute to fetch for the given key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      object\n",
            " |          Value of the additional attribute fetched for the given key.\n",
            " |  \n",
            " |  get_vector(self, key, norm=False)\n",
            " |      Get the key's vector, as a 1D numpy array.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Key for vector to return.\n",
            " |      norm : bool, optional\n",
            " |          If True, the resulting vector will be L2-normalized (unit Euclidean length).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      numpy.ndarray\n",
            " |          Vector for the specified key.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      \n",
            " |      KeyError\n",
            " |          If the given key doesn't exist.\n",
            " |  \n",
            " |  has_index_for(self, key)\n",
            " |      Can this model return a single index for this key?\n",
            " |      \n",
            " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
            " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
            " |      simple `word in wv` (`__contains__()`) check but False for this\n",
            " |      more-specific check.\n",
            " |  \n",
            " |  init_sims(self, replace=False)\n",
            " |      Precompute data helpful for bulk similarity calculations.\n",
            " |      \n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms` now preferred for this purpose.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      replace : bool, optional\n",
            " |          If True - forget the original vectors and only keep the normalized ones.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      \n",
            " |      You **cannot sensibly continue training** after doing a replace on a model's\n",
            " |      internal KeyedVectors, and a replace is no longer necessary to save RAM. Do not use this method.\n",
            " |  \n",
            " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
            " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
            " |      where it intersects with the current vocabulary.\n",
            " |      \n",
            " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
            " |      non-intersecting words are left alone.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          The file path to load the vectors from.\n",
            " |      lockf : float, optional\n",
            " |          Lock-factor value to be set for any imported word-vectors; the\n",
            " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
            " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
            " |      binary : bool, optional\n",
            " |          If True, `fname` is in the binary word2vec C format.\n",
            " |      encoding : str, optional\n",
            " |          Encoding of `text` for `unicode` function (python2 only).\n",
            " |      unicode_errors : str, optional\n",
            " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
            " |  \n",
            " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
            " |      Find the top-N most similar keys.\n",
            " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
            " |      \n",
            " |      This method computes cosine similarity between a simple mean of the projection\n",
            " |      weight vectors of the given keys and the vectors for each key in the model.\n",
            " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
            " |      word2vec implementation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
            " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
            " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
            " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all keys are returned.\n",
            " |      clip_start : int\n",
            " |          Start clipping index.\n",
            " |      clip_end : int\n",
            " |          End clipping index.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
            " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
            " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
            " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
            " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
            " |      In the common analogy-solving case, of two positive and one negative examples,\n",
            " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
            " |      \n",
            " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
            " |      respectively - a potentially sensible but untested extension of the method.\n",
            " |      With a single positive example, rankings will be the same as in the default\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
            " |      \n",
            " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
            " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      positive : list of str, optional\n",
            " |          List of words that contribute positively.\n",
            " |      negative : list of str, optional\n",
            " |          List of words that contribute negatively.\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all words are returned.\n",
            " |      restrict_vocab : int or None, optional\n",
            " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
            " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
            " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
            " |      \n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all words are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  most_similar_to_given(self, key1, keys_list)\n",
            " |      Get the `key` from `keys_list` most similar to `key1`.\n",
            " |  \n",
            " |  n_similarity(self, ws1, ws2)\n",
            " |      Compute cosine similarity between two sets of keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ws1 : list of str\n",
            " |          Sequence of keys.\n",
            " |      ws2: list of str\n",
            " |          Sequence of keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Similarities between `ws1` and `ws2`.\n",
            " |  \n",
            " |  rank(self, key1, key2)\n",
            " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
            " |  \n",
            " |  rank_by_centrality(self, words, use_norm=True)\n",
            " |      Rank the given words by similarity to the centroid of all the words.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      words : list of str\n",
            " |          List of keys.\n",
            " |      use_norm : bool, optional\n",
            " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (float, str)\n",
            " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
            " |  \n",
            " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
            " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
            " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
            " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
            " |      \n",
            " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
            " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
            " |      any arbitrary word pairs.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      wa: str\n",
            " |          Word for which we have to look top-n similar word.\n",
            " |      wb: str\n",
            " |          Word for which we evaluating relative cosine similarity with wa.\n",
            " |      topn: int, optional\n",
            " |          Number of top-n similar words to look with respect to wa.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.float64\n",
            " |          Relative cosine similarity between wa and wb.\n",
            " |  \n",
            " |  resize_vectors(self, seed=0)\n",
            " |      Make underlying vectors match index_to_key size; random-initialize any new rows.\n",
            " |  \n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save KeyedVectors to a file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the output file.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.load`\n",
            " |          Load a previously saved model.\n",
            " |  \n",
            " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
            " |      Store the input-hidden weight matrix in the same format used by the original\n",
            " |      C word2vec-tool, for compatibility.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          File path to save the vectors to.\n",
            " |      fvocab : str, optional\n",
            " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
            " |      binary : bool, optional\n",
            " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
            " |      total_vec : int, optional\n",
            " |          Explicitly specify total number of vectors\n",
            " |          (in case word vectors are appended with document vectors afterwards).\n",
            " |      write_header : bool, optional\n",
            " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
            " |          This is the format used by e.g. gloVe vectors.\n",
            " |      prefix : str, optional\n",
            " |          String to prepend in front of each stored word. Default = no prefix.\n",
            " |      append : bool, optional\n",
            " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
            " |      sort_attr : str, optional\n",
            " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
            " |  \n",
            " |  set_vecattr(self, key, attr, val)\n",
            " |      Set attribute associated with the given key to value.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Store the attribute for this vector key.\n",
            " |      attr : str\n",
            " |          Name of the additional attribute to store for the given key.\n",
            " |      val : object\n",
            " |          Value of the additional attribute to store for the given key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      None\n",
            " |  \n",
            " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          Key\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
            " |          the vector of similarity scores.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar keys by vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector : numpy.array\n",
            " |          Vector from which similarities are to be computed.\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all keys are returned.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
            " |      Compatibility alias for similar_by_key().\n",
            " |  \n",
            " |  similarity(self, w1, w2)\n",
            " |      Compute cosine similarity between two keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      w1 : str\n",
            " |          Input key.\n",
            " |      w2 : str\n",
            " |          Input key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Cosine similarity between `w1` and `w2`.\n",
            " |  \n",
            " |  similarity_unseen_docs(self, *args, **kwargs)\n",
            " |  \n",
            " |  sort_by_descending_frequency(self)\n",
            " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
            " |  \n",
            " |  unit_normalize_all(self)\n",
            " |      Destructively scale all vectors to unit-length.\n",
            " |      \n",
            " |      You cannot sensibly continue training after such a step.\n",
            " |  \n",
            " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
            " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
            " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
            " |      corpus:\n",
            " |      \n",
            " |      >>> from collections import Counter\n",
            " |      >>> import itertools\n",
            " |      >>>\n",
            " |      >>> from gensim.models import FastText\n",
            " |      >>> from gensim.test.utils import datapath, common_texts\n",
            " |      >>>\n",
            " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
            " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
            " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
            " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
            " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
            " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : iterable\n",
            " |          The keys that will be vectorized.\n",
            " |      allow_inference : bool, optional\n",
            " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
            " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
            " |      copy_vecattrs : bool, optional\n",
            " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
            " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
            " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
            " |          you should set `allow_inference=False`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
            " |          Vectors for all the given keys.\n",
            " |  \n",
            " |  wmdistance(self, document1, document2, norm=True)\n",
            " |      Compute the Word Mover's Distance between two documents.\n",
            " |      \n",
            " |      When using this code, please consider citing the following papers:\n",
            " |      \n",
            " |      * `Rémi Flamary et al. \"POT: Python Optimal Transport\"\n",
            " |        <https://jmlr.org/papers/v22/20-451.html>`_\n",
            " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
            " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      document1 : list of str\n",
            " |          Input document.\n",
            " |      document2 : list of str\n",
            " |          Input document.\n",
            " |      norm : boolean\n",
            " |          Normalize all word vectors to unit length before computing the distance?\n",
            " |          Defaults to True.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Word Mover's distance between `document1` and `document2`.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      This method only works if `POT <https://pypi.org/project/POT/>`_ is installed.\n",
            " |      \n",
            " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
            " |      will be returned.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      ImportError\n",
            " |          If `POT <https://pypi.org/project/POT/>`_  isn't installed.\n",
            " |  \n",
            " |  word_vec(self, *args, **kwargs)\n",
            " |      Compatibility alias for get_vector(); must exist so subclass calls reach subclass get_vector().\n",
            " |  \n",
            " |  words_closer_than(self, word1, word2)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
            " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      The information stored in the file is incomplete (the binary tree is missing),\n",
            " |      so while you can query for word similarity etc., you cannot continue training\n",
            " |      with a model loaded this way.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          The file path to the saved word2vec-format file.\n",
            " |      fvocab : str, optional\n",
            " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
            " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
            " |      binary : bool, optional\n",
            " |          If True, indicates whether the data is in binary word2vec format.\n",
            " |      encoding : str, optional\n",
            " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
            " |      unicode_errors : str, optional\n",
            " |          default 'strict', is a string suitable to be passed as the `errors`\n",
            " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
            " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
            " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
            " |      limit : int, optional\n",
            " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
            " |          None, means read all.\n",
            " |      datatype : type, optional\n",
            " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
            " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
            " |      no_header : bool, optional\n",
            " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
            " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
            " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
            " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
            " |          Loaded model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  cosine_similarities(vector_1, vectors_all)\n",
            " |      Compute cosine similarities between one vector and a set of other vectors.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector_1 : numpy.ndarray\n",
            " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
            " |      vectors_all : numpy.ndarray\n",
            " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
            " |  \n",
            " |  log_accuracy(section)\n",
            " |  \n",
            " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  index2entity\n",
            " |  \n",
            " |  index2word\n",
            " |  \n",
            " |  vectors_norm\n",
            " |  \n",
            " |  vocab\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
            " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
            " |      optionally log the event at `log_level`.\n",
            " |      \n",
            " |      Events are important moments during the object's life, such as \"model created\",\n",
            " |      \"model saved\", \"model loaded\", etc.\n",
            " |      \n",
            " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
            " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
            " |      but is useful during debugging and support.\n",
            " |      \n",
            " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
            " |      will not record events into `self.lifecycle_events` then.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      event_name : str\n",
            " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
            " |      event : dict\n",
            " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
            " |          Can be empty.\n",
            " |      \n",
            " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
            " |      \n",
            " |          - `datetime`: the current date & time\n",
            " |          - `gensim`: the current Gensim version\n",
            " |          - `python`: the current Python version\n",
            " |          - `platform`: the current platform\n",
            " |          - `event`: the name of this event\n",
            " |      log_level : int\n",
            " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  load(fname, mmap=None) from builtins.type\n",
            " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to file that contains needed object.\n",
            " |      mmap : str, optional\n",
            " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
            " |          via mmap (shared memory) using `mmap='r'.\n",
            " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.utils.SaveLoad.save`\n",
            " |          Save object to file.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      object\n",
            " |          Object loaded from `fname`.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      AttributeError\n",
            " |          When called on an object instance instead of class (this is a class method).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Might need to try and train w2v from zero or solve the _VERB _NOUN shyte of rus corpora"
      ],
      "metadata": {
        "id": "DC3mBdCv2PK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create custom class for using word2vec in pipeline"
      ],
      "metadata": {
        "id": "QwXbaKnDqHml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecTrans(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, word2vec_model):\n",
        "        self.word2vec_model = word2vec_model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        embeddings = []\n",
        "        for text in X:\n",
        "            tokens = text.split()  # Split text into tokens\n",
        "            text_embeddings = []\n",
        "            for token in tokens:   # not working since ruscorpora has _VERB _NOUN and base form of the word\n",
        "                if token in self.word2vec_model:\n",
        "                    text_embeddings.append(self.word2vec_model[token])\n",
        "            embeddings.append(text_embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "MRlLNdlvpXG7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create custom class for preprocessing stuff before pass it to word2vec"
      ],
      "metadata": {
        "id": "U-hU5TFhtUij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [self.preprocessor(text) for text in X]\n"
      ],
      "metadata": {
        "id": "oIFcgZnCtaSi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  tk = tokenizer.tokenize(get_prep_text(text).lower())\n",
        "  return ' '.join(tk)"
      ],
      "metadata": {
        "id": "hVI1Q6h1tjNa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a separate class for w2v pipeline"
      ],
      "metadata": {
        "id": "XtMYV3M1uYlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class W2VPipe(iPipe):\n",
        "  def __init__(self, preprocessor, w2v, estimator):\n",
        "    super().__init__(estimator)\n",
        "    self.preprocessor = preprocessor\n",
        "    self.w2v = w2v\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    self.model = Pipeline([\n",
        "        ('preprocess', self.preprocessor),\n",
        "        ('w2v', self.w2v),\n",
        "        ('clf', self.estimator)])\n",
        "\n",
        "    self.model.fit(X_train, y_train)\n",
        "    return self.model"
      ],
      "metadata": {
        "id": "iIXtRaJ1qN7R"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gather it all together"
      ],
      "metadata": {
        "id": "r2ayrO_DvYlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_transformer = Word2VecTrans(word2vec_model)\n",
        "preprop = Preprocessor(preprocessor=preprocess)\n",
        "svm_w2v = SVC(kernel='linear', random_state=42, gamma=\"auto\", probability=True, C=1.2)"
      ],
      "metadata": {
        "id": "v3hkLLj9rBRR"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_fail = w2v_transformer.transform(preprop.transform(text_data['text']))"
      ],
      "metadata": {
        "id": "D8-OhE4wwVB2"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pipe = W2VPipe(preprop, w2v_transformer, svm_w2v)\n",
        "new_pipe.fit_predict(text_data)"
      ],
      "metadata": {
        "id": "PwyBMa4ivVg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add fastText for multilanguage word2vec"
      ],
      "metadata": {
        "id": "z2EFW633psUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TBI"
      ],
      "metadata": {
        "id": "Z9lYuY4f1uhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural networks solution"
      ],
      "metadata": {
        "id": "sTYi4nuJB2A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import NN stuff"
      ],
      "metadata": {
        "id": "TMCtBxEsJFvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.utils import pad_sequences, to_categorical"
      ],
      "metadata": {
        "id": "RAOb_m7gIVf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "LgJOFcxcB5gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerocessing"
      ],
      "metadata": {
        "id": "jzuTUm2CCBVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data"
      ],
      "metadata": {
        "id": "1POaf7K8C-Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "EuLXF6KhM_hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_process(df):\n",
        "    X = df['text'].copy()\n",
        "    y = df['category'].copy()\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    return train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BxG3kH1UCCny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize"
      ],
      "metadata": {
        "id": "3d_JVJ1NCXF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_tokenize(x1, x2):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(x1)\n",
        "\n",
        "  x_train = tokenizer.texts_to_sequences(x1)\n",
        "  x_test = tokenizer.texts_to_sequences(x2)\n",
        "\n",
        "  return tokenizer, x_train, x_test"
      ],
      "metadata": {
        "id": "pzYvag2QCR0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add padding"
      ],
      "metadata": {
        "id": "th0cjU-yDWQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_padme(x1, x2, maxlen=100):\n",
        "  x_train = pad_sequences(x1, maxlen=maxlen, padding='post')\n",
        "  x_test = pad_sequences(x2, maxlen=maxlen, padding='post')\n",
        "  return x_train, x_test"
      ],
      "metadata": {
        "id": "_Gh0rzlmDXsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create model"
      ],
      "metadata": {
        "id": "55tAGeDeDoy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier:\n",
        "  def __init__(self, vocab_size, embedding_dim, maxlen):\n",
        "    self.vocab = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.maxlen = maxlen\n",
        "    self.model = self._create_model()\n",
        "\n",
        "  def _create_model(self)->Sequential:\n",
        "    '''creates lstm with embedding, rnn and classificator'''\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=self.vocab, output_dim=self.embedding_dim, input_length=self.maxlen))\n",
        "    model.add(LSTM(units=64))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "  def fit(self, X_train:np.array, y_train:np.array, batch_size:int=64, epochs:int=5)->None:\n",
        "    '''trains model'''\n",
        "\n",
        "    self.model.fit(X_train, y_train.astype(int), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  def predict(self, X_test:np.array)->np.array:\n",
        "    '''predics values for validation and test'''\n",
        "    print(\"PREDICTING\")\n",
        "    X_test_padded = pad_sequences(X_test, maxlen=self.maxlen)\n",
        "    return self.model.predict(X_test_padded)\n",
        "\n",
        "\n",
        "  def evaluate(self, pred:np.array, y_test:np.array)->None:\n",
        "    '''reports statistics'''\n",
        "    loss, accuracy = self.model.evaluate(pred, y_test)\n",
        "    pred = (pred > 0.5).astype(int)\n",
        "    print(f\"Loss: {loss}\\nAccuracy: {accuracy}\")\n",
        "    print(classification_report(y_test, pred))\n",
        "\n",
        "\n",
        "  def fit_predict(self, X_train, X_test, y_train, y_test, batch=64, epochs=5):\n",
        "    '''fits data into model, predicts values and reports sttistics'''\n",
        "\n",
        "    self.fit(X_train, y_train, batch, epochs)\n",
        "    y_pred = self.predict(X_test)\n",
        "    print('\\n\\n\\n\\n\\ntest binary\\n\\n\\n\\n')\n",
        "    # Convert predicted values to binary values (0 or 1)\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    self.evaluate(y_pred_binary, y_test)"
      ],
      "metadata": {
        "id": "dnGMRpJ9Edxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run it"
      ],
      "metadata": {
        "id": "CaDxAODHJMOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kX_train, kX_test, ky_train, ky_test = k_process(df)\n",
        "k_tokenizer, kX_train, kX_test = k_tokenize(kX_train, kX_test)\n",
        "kX_train, kXtest = k_padme(kX_train, kX_test)\n",
        "\n",
        "kX_train = np.array(kX_train)\n",
        "kX_test = np.array(kX_test)\n",
        "ky_train = np.array(ky_train)\n",
        "ky_test = np.array(ky_test)\n"
      ],
      "metadata": {
        "id": "aFuZLDqCJV8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_vocab_size = len(k_tokenizer.word_index) + 1    # num of unique words in all texts +1  for padding token\n",
        "k_embedding_dim = 100                             #vectors representing a word\n",
        "maxlen = 100                                      # sequence length after tokenization. too long - cut it"
      ],
      "metadata": {
        "id": "-xa12nSYJQ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kLSTM = LSTMClassifier(k_vocab_size, k_embedding_dim, maxlen)\n",
        "kLSTM.fit_predict(kX_train, kX_test, ky_train, ky_test, epochs=10)"
      ],
      "metadata": {
        "id": "wl6Tuq9kKA2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try multiclass"
      ],
      "metadata": {
        "id": "LEf6go_Q1f2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class M_LSTMTextClassifier:\n",
        "    def __init__(self, num_classes=4, vocab_size=10000, embedding_dim=128, lstm_units=128):\n",
        "        self.num_classes = num_classes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_units = lstm_units\n",
        "        self.model = self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))\n",
        "        model.add(LSTM(self.lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def preprocess_data(self, text_data, target):\n",
        "        tokenizer = Tokenizer(num_words=self.vocab_size)\n",
        "        tokenizer.fit_on_texts(text_data)\n",
        "        sequences = tokenizer.texts_to_sequences(text_data)\n",
        "        word_index = tokenizer.word_index\n",
        "\n",
        "        max_sequence_length = max([len(seq) for seq in sequences])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "        one_hot_target = tf.keras.utils.to_categorical(target, self.num_classes)\n",
        "\n",
        "        return padded_sequences, one_hot_target\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=32, validation_split=0.2):\n",
        "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        return self.model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "ndGL-mai_Xpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_X_y(df):\n",
        "  mtext_data = df['text']# .drop('category', axis=1) causes dimension problem when passed to the Sequential gotta find a way to solve it if added nontext cols\n",
        "  mtarget = df['category']\n",
        "  return mtext_data, mtarget\n",
        "\n",
        "def get_encoded_lstm(mtarget):\n",
        "  mlabel_encoder = LabelEncoder()\n",
        "  y = mlabel_encoder.fit_transform(mtarget)\n",
        "  return mlabel_encoder, y\n",
        "\n",
        "\n",
        "def split_lstm(params, target):\n",
        "  return train_test_split(params, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0L7JB9g3Vg0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(X_train, y_train, classnum, epochs = 10):\n",
        "  classifier = M_LSTMTextClassifier(num_classes=classnum, vocab_size=10000)\n",
        "  # Preprocess and train the model\n",
        "  X_train_processed, y_train_processed = classifier.preprocess_data(X_train, y_train)\n",
        "  classifier.train(X_train_processed, y_train_processed, epochs=epochs)\n",
        "  return classifier\n",
        "\n",
        "\n",
        "def test_lstm(classifier, label_encoder, mX_test, my_test):\n",
        "  X_test_processed, y_test_processed = classifier.preprocess_data(mX_test, my_test)\n",
        "  loss, accuracy = classifier.evaluate(X_test_processed, y_test_processed)\n",
        "  print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  # Convert predicted labels to original class names\n",
        "  y_pred = classifier.model.predict(X_test_processed)\n",
        "  predicted_labels = np.argmax(y_pred, axis=1)\n",
        "  predicted_class_names = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "  # Generate classification report\n",
        "  classification_rep = classification_report(y_test_processed.argmax(axis=1), predicted_labels, target_names=label_encoder.classes_)\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_rep)\n",
        "  return\n"
      ],
      "metadata": {
        "id": "p381taOdBG7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mX, my = lstm_X_y(text_data)\n",
        "multi_encoder, my_labels = get_encoded_lstm(my)\n",
        "mX_train, mX_test, my_train, my_test = split_lstm(mX, my_labels)\n",
        "\n",
        "mclassifier = train_lstm(mX_train, my_train, len(multi_encoder.classes_), 10)\n",
        "\n",
        "test_lstm(mclassifier, multi_encoder, mX_test, my_test)"
      ],
      "metadata": {
        "id": "ctG78p6njEz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}