{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMCvayRCjWN6jkGBuG8IYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Embeddings/word2vec_ods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec from scratch using pytorch"
      ],
      "metadata": {
        "id": "SnME0UvPrxe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P6DbazYprwoH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a general class to use it instead of global parameters"
      ],
      "metadata": {
        "id": "sjp_Xounv5k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "  \"\"\"\n",
        "  General metaclass for global parameters\n",
        "  \"\"\"\n",
        "  abs_window = 2\n",
        "  dims = 300\n",
        "  lr = 0.1\n",
        "  neg_size = 20\n",
        "  epochs = 10\n",
        "  zero = torch.tensor(0.0)\n",
        "  pattern = r\"[a-zа-яё0-9_]+\"\n",
        "\n"
      ],
      "metadata": {
        "id": "PvsmU3OItMdX"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "qmuQvcKvv-5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(filename: str) -> str:\n",
        "  with open(filename, 'rt', encoding='cp1251') as f:\n",
        "    text = ' '.join([line.strip() for line in f.readlines()[:-7]])\n",
        "  return text"
      ],
      "metadata": {
        "id": "0Z-aSOmRtd5g"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(inp: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess the data\n",
        "    \"\"\"\n",
        "\n",
        "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
        "    inp = inp.lower()\n",
        "    inp = re.sub('[^ 0-9a-яА-Я]+', ' ', inp)\n",
        "    inp = re.sub(r'\\s+', ' ', inp)\n",
        "    return inp\n"
      ],
      "metadata": {
        "id": "Q5Z4Fp6Dr37V"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Looks like we won't make it without proper tokenization"
      ],
      "metadata": {
        "id": "zlJafvJsRczE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, txt):\n",
        "    self.word_pattern = re.compile(CFG.pattern)\n",
        "    self.vocab = self._get_vocab_list(txt)\n",
        "    self.idx_word_dict = self._get_idx_n_words\n",
        "\n",
        "\n",
        "  def tokenize(self, txt):\n",
        "    return list(map(lambda x: self.idx_word_dict[x], self._get_re_result(txt)))\n",
        "\n",
        "  @property\n",
        "  def _get_idx_n_words(self):\n",
        "    return dict(zip(self.vocab, range(len(self.vocab))))\n",
        "\n",
        "\n",
        "  def _get_re_result(self, txt):\n",
        "    cleaned = clean(txt).lower()\n",
        "    result = self.word_pattern.findall(cleaned)\n",
        "    return result\n",
        "\n",
        "  def _get_vocab_list(self, txt):\n",
        "    result = self._get_re_result(txt)\n",
        "    return list(set(result))"
      ],
      "metadata": {
        "id": "2pxdg0guRu9g"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skipgram"
      ],
      "metadata": {
        "id": "DDtdx_CqnC4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, size: int):\n",
        "        super().__init__()\n",
        "        self.embeddings = torch.nn.Embedding(size, CFG.dims)\n",
        "\n",
        "    def forward(self, target):\n",
        "        return self.embeddings(target)\n"
      ],
      "metadata": {
        "id": "KuSGEAcanCRO"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str) -> Dict[str, np.array]:\n",
        "  \"\"\"\n",
        "  Train the word2vec model using SkipGram approach\n",
        "  return a dict:\n",
        "    -key: a word itself\n",
        "    -value: embeddings\n",
        "  \"\"\"\n",
        "\n",
        "  tokenizer = Tokenizer(data)\n",
        "  tokens = tokenizer.tokenize(data)\n",
        "  model = SkipGram(len(tokenizer.vocab))\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "  criterion = lambda x: -x\n",
        "\n",
        "  for epoch in range(CFG.epochs):\n",
        "    curr_loss = []\n",
        "\n",
        "    for idx, center_word in enumerate(tokens):\n",
        "      optimizer.zero_grad()\n",
        "      # get context tokens for around the center word (-2:+2) excluding center word\n",
        "      context_idxs = torch.LongTensor([tokens[i] for i in range(\n",
        "                                      max(0, idx-CFG.abs_window),\n",
        "                                      min(len(tokens), idx+CFG.abs_window)\n",
        "                                      ) if i != idx])\n",
        "\n",
        "      # get negative samples\n",
        "      neg_idxs = torch.randint(0, len(tokenizer.vocab), (CFG.neg_size,))\n",
        "\n",
        "      # get embeddings for center word\n",
        "      center_emb = model(torch.LongTensor([center_word]))\n",
        "      # get embeddings for context words\n",
        "      context_embs = model(context_idxs)\n",
        "      # get embeddings for neg samples\n",
        "      neg_embs = model(neg_idxs)\n",
        "\n",
        "      # get pos dist of cos similarity\n",
        "      pos = (1-F.cosine_similarity(center_emb, context_embs)).mean()\n",
        "      # get neg distance\n",
        "      neg = (1-F.cosine_similarity(center_emb, neg_embs)).mean()\n",
        "\n",
        "      # calculate loss\n",
        "      loss = torch.max(CFG.zero, pos-neg+1) #margin is 1\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      curr_loss.append(loss.item())\n",
        "    loss=loss.mean()\n",
        "    print('loss:', np.mean(curr_loss))\n",
        "\n",
        "  result_weights = model.embeddings.weight.data.numpy()\n",
        "  result = dict(zip(tokenizer.idx_word_dict.keys(), result_weights))\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "iV3bYAnunwFH"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'A quick brown fox jumps over a lazy dog! Hi, is it your string?'\n",
        "result = train(clean(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgD1kE-S-Ypl",
        "outputId": "22c30b03-188f-449b-f9fa-669e7d77d0bf"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.0325896995408195\n",
            "loss: 0.6622783711978367\n",
            "loss: 0.37945176873888287\n",
            "loss: 0.2823371844632285\n",
            "loss: 0.23547078456197465\n",
            "loss: 0.16382697224617004\n",
            "loss: 0.23219232048307145\n",
            "loss: 0.20551985927990504\n",
            "loss: 0.2082888696874891\n",
            "loss: 0.17689756836209977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train on a larger corpus"
      ],
      "metadata": {
        "id": "2eMZrS50_wOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = clean(data_loader('master.txt'))\n",
        "train(text)  # takes too long"
      ],
      "metadata": {
        "id": "zrgnMHMRuKXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}