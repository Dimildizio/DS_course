{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWug9wSoz33v6pTLaMdTv6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Embeddings/word2vec_ods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec from scratch using pytorch"
      ],
      "metadata": {
        "id": "SnME0UvPrxe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P6DbazYprwoH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a general class to use it instead of global parameters"
      ],
      "metadata": {
        "id": "sjp_Xounv5k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "  \"\"\"\n",
        "  General metaclass for global parameters\n",
        "  \"\"\"\n",
        "  window_size = 10\n",
        "  dims = 300\n",
        "  lr = 0.01\n",
        "  neg_size = 20\n",
        "  epochs = 10\n",
        "  pattern = r\"[a-zа-яё0-9_]+\"\n"
      ],
      "metadata": {
        "id": "PvsmU3OItMdX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "qmuQvcKvv-5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(inp: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess the data\n",
        "    \"\"\"\n",
        "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
        "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
        "    return inp\n"
      ],
      "metadata": {
        "id": "Q5Z4Fp6Dr37V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids(data):\n",
        "  \"\"\"\n",
        "  Create unique values and get enumerated dictionary\n",
        "  \"\"\"\n",
        "  vocab = set(clean(data).split())\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  enum_words = dict(zip(range(vocab_size), vocab))\n",
        "  return enum_words"
      ],
      "metadata": {
        "id": "8nYwcrN11EFU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Looks like we won't make it without proper tokenization"
      ],
      "metadata": {
        "id": "zlJafvJsRczE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, txt):\n",
        "    self.word_pattern = re.compile(CFG.pattern)\n",
        "    self.vocab = self._get_vocab_list(txt)\n",
        "    self.idx_word_dict = self._get_idx_n_words\n",
        "\n",
        "\n",
        "  def tokenize(self, txt):\n",
        "    return list(map(lambda x: self.idx_word_dict[x], self._get_re_result(txt)))\n",
        "\n",
        "  @property\n",
        "  def _get_idx_n_words(self):\n",
        "    return dict(zip(self.vocab, range(len(self.vocab))))\n",
        "\n",
        "\n",
        "  def _get_re_result(self, txt):\n",
        "    cleaned = clean(txt).lower()\n",
        "    result = self.word_pattern.findall(cleaned)\n",
        "    return result\n",
        "\n",
        "  def _get_vocab_list(self, txt):\n",
        "    result = self._get_re_result(txt)\n",
        "    return list(set(result))"
      ],
      "metadata": {
        "id": "2pxdg0guRu9g"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Word2Vec class"
      ],
      "metadata": {
        "id": "q9c_erkA2WkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "\n",
        "    def __init__(self, size: int):\n",
        "        super().__init__()\n",
        "        self.vocab_size = size\n",
        "        self.center_emb = torch.nn.Embedding(size, CFG.dims)\n",
        "        self.context_emb = torch.nn.Embedding(size, CFG.dims)\n",
        "\n",
        "    def forward(self, X):\n",
        "      middle_idx = len(X) // 2\n",
        "      middle_word = torch.LongTensor([X[middle_idx]])\n",
        "      center_embeddings = self.center_emb(middle_word).flatten()\n",
        "\n",
        "      contexts = torch.LongTensor(X[:middle_idx] + X[:middle_idx+1])\n",
        "      context_embeddings = self.context_emb(contexts)\n",
        "      noise = torch.randint(0, self.vocab_size, (CFG.neg_size,))\n",
        "      noise_embeddings = self.context_emb(noise)\n",
        "\n",
        "      pos_score = F.logsigmoid(context_embeddings @ center_embeddings)\n",
        "      neg_score = F.logsigmoid(-noise_embeddings @ center_embeddings)\n",
        "\n",
        "      result = pos_score.sum() + neg_score.sum()\n",
        "      return result"
      ],
      "metadata": {
        "id": "GWUqsOe22YY4"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function to train Word2Vec"
      ],
      "metadata": {
        "id": "qp1RAj9ywMLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data: str) -> Dict[str, np.array]:\n",
        "    \"\"\"\n",
        "    Train the word2vec model\n",
        "    return a dict:\n",
        "     -key: a word itself\n",
        "     -value: embeddings\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(data)\n",
        "    tokens = tokenizer.tokenize(data)\n",
        "    model = Word2Vec(len(tokenizer.vocab))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
        "    criterion = lambda x: -x\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "      curr_loss = float(0)\n",
        "      token_windows = len(tokens) - CFG.window_size+1\n",
        "      for idx in range(token_windows):\n",
        "        optimizer.zero_grad()\n",
        "        window = tokens[idx:idx+CFG.window_size]  # requires a fix\n",
        "        loss = criterion(model(window))\n",
        "        loss.backward()\n",
        "        curr_loss += loss.item()\n",
        "\n",
        "      result_weights = model.context_emb.weight.data.numpy()\n",
        "      result = dict(zip(tokenizer.idx_word_dict.keys(), result_weights))\n",
        "    return result"
      ],
      "metadata": {
        "id": "AOqVk3sUsFDw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the functionality"
      ],
      "metadata": {
        "id": "sd7oaGsFwTWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'A quick brown fox jumps over a lazy dog! Hi, is it your string?'"
      ],
      "metadata": {
        "id": "Da5CWjyPsGjw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = train(data)"
      ],
      "metadata": {
        "id": "DKqc29SLyadf"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skipgram"
      ],
      "metadata": {
        "id": "DDtdx_CqnC4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "\n",
        "    def __init__(self, size: int):\n",
        "        super().__init__()\n",
        "        self.vocab_size = size\n",
        "        self.center_emb = torch.nn.Embedding(size, CFG.dims)\n",
        "        self.context_emb = torch.nn.Embedding(size, CFG.dims)\n",
        "\n",
        "    def forward(self, target, context):\n",
        "        center_word = torch.LongTensor([target])\n",
        "        center_embedding = self.center_emb(center_word).flatten()\n",
        "\n",
        "        context_words = torch.LongTensor([context])\n",
        "        context_embedding = self.context_emb(context_words)\n",
        "\n",
        "        score = F.logsigmoid(context_embedding @ center_embedding)\n",
        "\n",
        "        return score\n",
        "\n"
      ],
      "metadata": {
        "id": "KuSGEAcanCRO"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_skipgram(data: str) -> Dict[str, np.array]:\n",
        "    \"\"\"\n",
        "    Train the word2vec model using SkipGram approach\n",
        "    return a dict:\n",
        "     -key: a word itself\n",
        "     -value: embeddings\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(data)\n",
        "    tokens = tokenizer.tokenize(data)\n",
        "    model = SkipGram(len(tokenizer.vocab))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    criterion = lambda x: -x\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "      curr_loss = float(0)\n",
        "      token_windows = len(tokens) - CFG.window_size+1\n",
        "      for idx in range(token_windows):\n",
        "          for context_idx in range(max(0, idx - CFG.window_size), min(token_windows, idx + CFG.window_size + 1)):\n",
        "              optimizer.zero_grad()\n",
        "              target = tokens[context_idx]\n",
        "              context = tokens[idx]\n",
        "              loss = criterion(model(target, context))\n",
        "              loss.backward()\n",
        "              curr_loss += loss.item()\n",
        "\n",
        "      result_weights = model.context_emb.weight.data.numpy()\n",
        "      result = dict(zip(tokenizer.idx_word_dict.keys(), result_weights))\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "iV3bYAnunwFH"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_skipgram(data)"
      ],
      "metadata": {
        "id": "TSXwYO8Hn-1V"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}