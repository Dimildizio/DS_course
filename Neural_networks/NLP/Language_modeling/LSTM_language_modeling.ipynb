{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtn68cKYckXJmsbbnA1NL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Language_modeling/LSTM_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM language modeling"
      ],
      "metadata": {
        "id": "WeMsVoW8uLSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "RN0gnfQQuOll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0cUpAuiuGew"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "import seaborn\n",
        "seaborn.set(palette='summer')"
      ],
      "metadata": {
        "id": "s33wN5XKuRj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "I5C7-YiFuTr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "-G6DFqokuXO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "AUHzS8WNubu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "bnKcLX7muZLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and creating vocab"
      ],
      "metadata": {
        "id": "kMPJVwz1uh-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Preprocess\n",
        "\n",
        "> Get vocab or `set` of strings:\n",
        "\n",
        "  1. split train samples into separate sentences using `sent_tokenize` from `nltk`. Each separate sentence would be a single instance of training samples.\n",
        "\n",
        "  2. Drop sentences with words num **above** `word_threshold`\n",
        "\n",
        "  3. Count each word in resulting sentences (Document Frequency). (use `word_tokenize` to split into separate words)\n",
        "\n",
        "  4. Create `vocab` object of `set`, put `<unk>, <bos>, <eos>, <pad>` and `vocab_size` of the most frequent words."
      ],
      "metadata": {
        "id": "hjoA4H-xuqPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get separate sentences and put them in list"
      ],
      "metadata": {
        "id": "3XTyLS2Sv6Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "word_threshold = 32"
      ],
      "metadata": {
        "id": "ETnf_1iquo2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cound frequency"
      ],
      "metadata": {
        "id": "sulqF1sTwGB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = Counter()"
      ],
      "metadata": {
        "id": "ajKWfaxwwHTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add vocab_size of the most frequent words into vocab"
      ],
      "metadata": {
        "id": "m64tf3t1wKZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "vocab_size = 40000"
      ],
      "metadata": {
        "id": "r0oRW_BkwKoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bathe in tests"
      ],
      "metadata": {
        "id": "Q1eNz6JYwTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert '<unk>' in vocab\n",
        "assert '<bos>' in vocab\n",
        "assert '<eos>' in vocab\n",
        "assert '<pad>' in vocab\n",
        "assert len(vocab) == vocab_size + 4"
      ],
      "metadata": {
        "id": "MKp8ucyuwRKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total words in vocab:\", len(vocab))"
      ],
      "metadata": {
        "id": "KswCOkRJwVWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ],
      "metadata": {
        "id": "f_nvLn07wXP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create `__getitem__` (return data sample by input idx) in `WordDataset`.\n",
        "\n",
        "add service tokens for the beginning and the end of sequence and tokenize the sentence using `word_tokenize` and match it with indices from `word2idx`"
      ],
      "metadata": {
        "id": "qmWBZI3mwcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2ind = {char: i for i, char in enumerate(vocab)}\n",
        "ind2word = {i: char for char, i in word2ind.items()}"
      ],
      "metadata": {
        "id": "mOwcALPRw5b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordDataset:\n",
        "    def __init__(self, sentences):\n",
        "        self.data = sentences\n",
        "        self.unk_id = word2ind['<unk>']\n",
        "        self.bos_id = word2ind['<bos>']\n",
        "        self.eos_id = word2ind['<eos>']\n",
        "        self.pad_id = word2ind['<pad>']\n",
        "\n",
        "    def __getitem__(self, idx: int) -> List[int]:\n",
        "        tokenized_sentence = []\n",
        "        # MOAR CODE HERE\n",
        "\n",
        "        return tokenized_sentence\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "f5wOMRZNw5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_with_padding(\n",
        "    input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> torch.Tensor:\n",
        "    seq_lens = [len(x) for x in input_batch]\n",
        "    max_seq_len = max(seq_lens)\n",
        "\n",
        "    new_batch = []\n",
        "    for sequence in input_batch:\n",
        "        for _ in range(max_seq_len - len(sequence)):\n",
        "            sequence.append(pad_id)\n",
        "        new_batch.append(sequence)\n",
        "\n",
        "    sequences = torch.LongTensor(new_batch).to(device)\n",
        "\n",
        "    new_batch = {\n",
        "        'input_ids': sequences[:,:-1],\n",
        "        'target_ids': sequences[:,1:]\n",
        "    }\n",
        "\n",
        "    return new_batch"
      ],
      "metadata": {
        "id": "2ShaGrhQw_YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)\n",
        "eval_sentences, test_sentences = train_test_split(sentences, test_size=0.5)\n",
        "\n",
        "train_dataset = WordDataset(train_sentences)\n",
        "eval_dataset = WordDataset(eval_sentences)\n",
        "test_dataset = WordDataset(test_sentences)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "TJ2A7mDuxBN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture and training"
      ],
      "metadata": {
        "id": "eu6vjAphxBjb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NepJqp15xFdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}