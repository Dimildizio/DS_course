{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPFsXNuuRyq043AhADFAu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/NLP/Language_modeling/LSTM_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM language modeling"
      ],
      "metadata": {
        "id": "WeMsVoW8uLSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "RN0gnfQQuOll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0cUpAuiuGew"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "import seaborn\n",
        "seaborn.set(palette='summer')"
      ],
      "metadata": {
        "id": "s33wN5XKuRj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "I5C7-YiFuTr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "-G6DFqokuXO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "AUHzS8WNubu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "bnKcLX7muZLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and creating vocab"
      ],
      "metadata": {
        "id": "kMPJVwz1uh-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Preprocess\n",
        "\n",
        "> Get vocab or `set` of strings:\n",
        "\n",
        "  1. split train samples into separate sentences using `sent_tokenize` from `nltk`. Each separate sentence would be a single instance of training samples.\n",
        "\n",
        "  2. Drop sentences with words num **above** `word_threshold`\n",
        "\n",
        "  3. Count each word in resulting sentences (Document Frequency). (use `word_tokenize` to split into separate words)\n",
        "\n",
        "  4. Create `vocab` object of `set`, put `<unk>, <bos>, <eos>, <pad>` and `vocab_size` of the most frequent words."
      ],
      "metadata": {
        "id": "hjoA4H-xuqPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get separate sentences and put them in list"
      ],
      "metadata": {
        "id": "3XTyLS2Sv6Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "word_threshold = 32"
      ],
      "metadata": {
        "id": "ETnf_1iquo2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cound frequency"
      ],
      "metadata": {
        "id": "sulqF1sTwGB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = Counter()"
      ],
      "metadata": {
        "id": "ajKWfaxwwHTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add vocab_size of the most frequent words into vocab"
      ],
      "metadata": {
        "id": "m64tf3t1wKZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "vocab_size = 40000"
      ],
      "metadata": {
        "id": "r0oRW_BkwKoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bathe in tests"
      ],
      "metadata": {
        "id": "Q1eNz6JYwTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert '<unk>' in vocab\n",
        "assert '<bos>' in vocab\n",
        "assert '<eos>' in vocab\n",
        "assert '<pad>' in vocab\n",
        "assert len(vocab) == vocab_size + 4"
      ],
      "metadata": {
        "id": "MKp8ucyuwRKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total words in vocab:\", len(vocab))"
      ],
      "metadata": {
        "id": "KswCOkRJwVWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ],
      "metadata": {
        "id": "f_nvLn07wXP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create `__getitem__` (return data sample by input idx) in `WordDataset`.\n",
        "\n",
        "add service tokens for the beginning and the end of sequence and tokenize the sentence using `word_tokenize` and match it with indices from `word2idx`"
      ],
      "metadata": {
        "id": "qmWBZI3mwcY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {char: i for i, char in enumerate(vocab)}\n",
        "idx2word = {i: char for char, i in word2idx.items()}"
      ],
      "metadata": {
        "id": "mOwcALPRw5b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordDataset:\n",
        "  def __init__(self, sent):\n",
        "    self.data = sent\n",
        "    self.unk_id, self.bos_id, self.eos_id, self.pad_id  = [word2idx[tag] for tag in tags]\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx: int) -> dict:\n",
        "    processed_txt = self.data[idx]['text'].lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tok_sent = [self.bos_id]\n",
        "    tok_sent += [word2idx.get(word, self.unk_id) for word in word_tokenize(processed_txt)]\n",
        "    tok_sent += [self.eos_id]\n",
        "\n",
        "    batch = {'text': tok_sent, 'label': self.data[idx]['label']}\n",
        "    return batch\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.data)\n"
      ],
      "metadata": {
        "id": "f5wOMRZNw5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_with_padding(input_batch: List[List[int]],\n",
        "                            pad_id=word2idx['<pad>']) -> torch.Tensor:\n",
        "\n",
        "    seq_lens = [len(x) for x in input_batch]\n",
        "    max_seq_len = max(seq_lens)\n",
        "\n",
        "    new_batch = []\n",
        "    for seq in input_batch:\n",
        "        for _ in range(max_seq_len - len(seq)):\n",
        "            seq.append(pad_id)\n",
        "        new_batch.append(seq)\n",
        "\n",
        "    sequences = torch.LongTensor(new_batch).to(device)\n",
        "\n",
        "    new_batch = {'input_ids': sequences[:,:-1], 'target_ids': sequences[:,1:]}\n",
        "    return new_batch\n"
      ],
      "metadata": {
        "id": "2ShaGrhQw_YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)\n",
        "eval_sentences, test_sentences = train_test_split(sentences, test_size=0.5)\n",
        "\n",
        "train_dataset = WordDataset(train_sentences)\n",
        "eval_dataset = WordDataset(eval_sentences)\n",
        "test_dataset = WordDataset(test_sentences)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "TJ2A7mDuxBN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture and training"
      ],
      "metadata": {
        "id": "eu6vjAphxBjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline"
      ],
      "metadata": {
        "id": "jBxOOiv4jP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, vocab_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_dim, rnn_layers=2, bi=True)\n",
        "        self.rnn = nn.LSTM(input_size=vocab_dim, hidden_size=vocab_dim, num_layers=rnn_layers, batch_first=True, bidirectional=bi)\n",
        "        self.fc_1 = nn.Linear(vocab_dim*(bi+1), vocab_dim)\n",
        "        self.fc_2 = nn.Linear(vocab_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        x, _ = self.rnn(embedding)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.fc_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NepJqp15xFdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation func (1 point)\n",
        "\n",
        "perplexity is an exponent applied to cross-entropy loss"
      ],
      "metadata": {
        "id": "riK52sDCSS4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, criterion, dataloader) -> float:\n",
        "    model.eval()\n",
        "    perplexity = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X = batch[\"input_ids\"]\n",
        "            logits =model(X)\n",
        "            loss = criterion(logits, batch['target_ids'].flatten())\n",
        "            perplexity.append(torch.exp(loss).item())\n",
        "\n",
        "    perplexity = sum(perplexity) / len(perplexity)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "XUlMUVJ3mL4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train func (1 point)"
      ],
      "metadata": {
        "id": "UEpeiTCSUKfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_epoch(model, optimizer, criterion, epoch, loader, name='Train', scheduler=False):\n",
        "  if name=='Train':\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()\n",
        "  avg_loss = 0\n",
        "  avg_perplexity = 0\n",
        "  for batch in tqdm(loader, desc=f'{name} Epoch: {epoch}}'):\n",
        "    xs, ys = batch['input_ids'], batch['target_ids']\n",
        "    logits = model(xs)\n",
        "    loss = criterion(logits.flatten(start_dim=0, end_dim=1), ys.flatten())\n",
        "    if name=='Train':\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    avg.loss += loss.item()\n",
        "    avg.perplexity += torch.exp(loss).item()\n",
        "    if scheduler:\n",
        "     scheduler.step()\n",
        "  return avg_loss / len(loader), avg_perplexity / len(loader)\n",
        "\n",
        "\n",
        "def fit(model, optimizer, criterion, train_loader, eval_loader, scheduler):\n",
        "  hist_loss_train, hist_loss_val, hist_perp_train, hist_perp_val = [] * 4\n",
        "  for epoch in tqdm(CFG.epochs):\n",
        "    train_loss, train_perp = get_epoch(model, optimizer, criterion, epoch, train_loader, scheduler=scheduler)\n",
        "    hist_loss_train.append(train_loss)\n",
        "    hist_perp_train.append(train_perp)\n",
        "\n",
        "    val_loss, val_perp = get_epoch(model, optimizer, criterion, epoch, eval_loader, name='Val' scheduler=scheduler)\n",
        "    hist_loss_val.append(val_loss)\n",
        "    hist_perp_val.append(val_perp)\n",
        "\n",
        "\n",
        "    t_loss = round(hist_train_loss[-1], 5)\n",
        "    v_loss = round(hist_test_loss[-1], 5)\n",
        "    t_perplexity = round(hist_train_perplexity[-1], 5)\n",
        "    v_perplexity = round(hist_test_perplexity[-1], 5)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}.\\n\\tTrain loss: {t_loss}\\n\\ttest_loss: {v_loss}\"\n",
        "          f\"\\n\\ttrain_perplexity: {t_perplexity}\\n\\ttest_perplexity: {v_perplexity}\")\n",
        "\n",
        "  return hist_loss_train, hist_loss_val, hist_perp_train, hist_perp_val"
      ],
      "metadata": {
        "id": "8XnW_qyvXIlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}