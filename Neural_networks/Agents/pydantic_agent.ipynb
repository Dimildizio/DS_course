{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5PlOciRSWdqIkcawV6evE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/Agents/pydantic_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSsF4jJv_OM7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install 'pydantic-ai-slim[openai]'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Basic model connector\n"
      ],
      "metadata": {
        "id": "gsd-lJXxFq8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and env variables\n",
        "\n",
        "First we import libs and create env variable for `OpenAIChatModel` to be able to get `OPENROUTER_API_KEY` from env variables"
      ],
      "metadata": {
        "id": "V0skyl4KJQIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from pydantic_ai.models.openai import OpenAIChatModel\n",
        "from pydantic_ai.providers.openrouter import OpenRouterProvider\n",
        "\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get('openrouter')\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY"
      ],
      "metadata": {
        "id": "ok0WrhoM_gy6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating model and agent\n"
      ],
      "metadata": {
        "id": "csrqrPYKGcuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an agent output scheme"
      ],
      "metadata": {
        "id": "VVCre3yXGWmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Result(BaseModel):\n",
        "    answer: str\n",
        "    confidence: float"
      ],
      "metadata": {
        "id": "vlU2QgxF_pv4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Write system prompt\n",
        "\n",
        "> create model\n",
        "\n",
        "> create agent"
      ],
      "metadata": {
        "id": "gS4CEynmJXpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"You're a study assistant agent. Your answer should be brief and structured. Avoid emoji and slang.\"\n",
        "\n",
        "model = OpenAIChatModel(\"deepseek/deepseek-chat-v3.1:free\",\n",
        "                        provider=OpenRouterProvider())\n",
        "\n",
        "\n",
        "agent = Agent[None, Result](model=model,\n",
        "                            system_prompt=sys_prompt)"
      ],
      "metadata": {
        "id": "iAxFdvZw_dD6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling `agent.run` and memory\n",
        "\n",
        "Simple async run function with an agent decorator. In Pydantic AI the context \"lives\" while the `agent.run` is being executed (unlike usual LLM request `model.complete` or `model.generate` when it doesn't have access to it's own history/tool use), however between requests there is no memory.\n",
        "\n",
        "The agent has access to `ctx.deps` - like ram and `ctx.memory_history` - all prompts to the model in current run, after `agent.run` return result the session is closed and the memory is erased."
      ],
      "metadata": {
        "id": "HgQCXYu3Go1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.run\n",
        "async def run(ctx: RunContext[None], question: str) -> Result:\n",
        "    draft = await ctx.llm.complete(question)\n",
        "    return Result(answer=draft.data, confidence=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXEYA24WAIMp",
        "outputId": "18a77285-053d-498e-e39e-fa091a8c4d70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1231992350.py:2: RuntimeWarning: coroutine 'AbstractAgent.run' was never awaited\n",
            "  async def run(ctx: RunContext[None], question: str) -> Result:\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try it"
      ],
      "metadata": {
        "id": "jslYkqdfGtwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run(\"Привет, кто ты?\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NVCKX_vAn-l",
        "outputId": "64b849ae-47c5-4d77-ac9b-977793b9ab39"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentRunResult(output='Я — ваш помощник-ассистент, предназначенный для ответов на вопросы и помощи в учёбе. Чем могу помочь?')\n"
          ]
        }
      ]
    }
  ]
}