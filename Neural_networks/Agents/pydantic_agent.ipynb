{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjzd4mzs7rr/wwLKzoV1K4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimildizio/DS_course/blob/main/Neural_networks/Agents/pydantic_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zSsF4jJv_OM7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install 'pydantic-ai-slim[openai]'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Basic model connector\n"
      ],
      "metadata": {
        "id": "gsd-lJXxFq8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and env variables\n",
        "\n",
        "First we import libs and create env variable for `OpenAIChatModel` to be able to get `OPENROUTER_API_KEY` from env variables"
      ],
      "metadata": {
        "id": "V0skyl4KJQIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "from pydantic_ai import Agent, RunContext\n",
        "from pydantic_ai.models.openai import OpenAIChatModel\n",
        "from pydantic_ai.providers.openrouter import OpenRouterProvider\n",
        "\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get('openrouter')\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY"
      ],
      "metadata": {
        "id": "ok0WrhoM_gy6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiating model and agent\n"
      ],
      "metadata": {
        "id": "csrqrPYKGcuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an agent output scheme"
      ],
      "metadata": {
        "id": "VVCre3yXGWmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Result(BaseModel):\n",
        "    answer: str\n",
        "    confidence: float"
      ],
      "metadata": {
        "id": "vlU2QgxF_pv4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Write system prompt\n",
        "\n",
        "> create model\n",
        "\n",
        "> create agent"
      ],
      "metadata": {
        "id": "gS4CEynmJXpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"You're a study assistant agent. Your answer should be brief and structured. Avoid emoji and slang.\"\n",
        "\n",
        "model = OpenAIChatModel(\"deepseek/deepseek-chat-v3.1:free\",\n",
        "                        provider=OpenRouterProvider())\n",
        "\n",
        "\n",
        "agent = Agent[None, Result](model=model,\n",
        "                            system_prompt=sys_prompt)"
      ],
      "metadata": {
        "id": "iAxFdvZw_dD6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling `agent.run` and memory\n",
        "\n",
        "Simple async run function with an agent decorator. In Pydantic AI the context \"lives\" while the `agent.run` is being executed (unlike usual LLM request `model.complete` or `model.generate` when it doesn't have access to it's own history/tool use), however between requests there is no memory.\n",
        "\n",
        "The agent has access to `ctx.deps` - like ram and `ctx.memory_history` - all prompts to the model in current run, after `agent.run` return result the session is closed and the memory is erased."
      ],
      "metadata": {
        "id": "HgQCXYu3Go1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.run\n",
        "async def run(ctx: RunContext[None], question: str) -> Result:\n",
        "    draft = await ctx.llm.complete(question)\n",
        "    return Result(answer=draft.data, confidence=0.5)"
      ],
      "metadata": {
        "id": "CXEYA24WAIMp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try it"
      ],
      "metadata": {
        "id": "jslYkqdfGtwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run(\"Hello, who are you?\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NVCKX_vAn-l",
        "outputId": "3265ee23-78ed-431e-b11e-867b4995465c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentRunResult(output='I am an AI assistant designed to help answer questions and provide support. How can I assist you today?')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: add tools"
      ],
      "metadata": {
        "id": "Yl_5HvUvGveq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More imports"
      ],
      "metadata": {
        "id": "zEJS9pwyJcgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, Optional, List\n",
        "from pydantic import Field\n",
        "import ast, operator as op"
      ],
      "metadata": {
        "id": "iZERBMTWG20-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create schema for dependencies - data accessible to the Agent during agent.run\n",
        "We create pydantic model so it would follow typisation, and get validated\n",
        "When `agent.run` is executed `Deps` gets instantiated and is accessible via `ctx.deps`"
      ],
      "metadata": {
        "id": "szaEso2KJgiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Deps(BaseModel):\n",
        "    log: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "\n",
        "class Result(BaseModel):\n",
        "    answer: str\n",
        "    confidence: float"
      ],
      "metadata": {
        "id": "bkj6ah0OHDIt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a new system prompt and agent"
      ],
      "metadata": {
        "id": "AvB5esvtG9sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"\"\"You are a calculation-and-text utility agent.\n",
        "Decide if a TOOL is needed. If so, emit a single JSON command with:\n",
        "  {\"tool\": \"count_word_chars\"|\"calc_expr\"|\"convert_units\"|\"none\", \"args\": {...}}\n",
        "Then STOP. The python runtime will execute the tool and show you its result.\n",
        "After you see the tool RESULT, produce the final short answer.\n",
        "\n",
        "Guidelines:\n",
        "- If the user asks to count characters per word -> choose \"count_word_chars\" and pass ONLY the target text as args: {\"text\": \"...\"}.\n",
        "- If it's a math expression (e.g., \"3*(2+5)/7\" or \"2**10 + 0.5\") -> choose \"calc_expr\" with {\"expression\": \"...\"}.\n",
        "- If it's a unit conversion like \"<number> <unit> to <unit>\" -> choose \"convert_units\" with {\"query\": \"...\"}.\n",
        "- Keep answers short and precise. If numeric, one line with the number and brief explanation.\n",
        "- Do NOT invent data. Prefer tools when applicable.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "agent = Agent[Deps, Result](model=model,\n",
        "                            system_prompt=sys_prompt)"
      ],
      "metadata": {
        "id": "1fxUrtT9OIP0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 1: calculator"
      ],
      "metadata": {
        "id": "HOgEN4jdPWdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CalcInput(BaseModel):\n",
        "    expression: str\n",
        "\n",
        "class CalcOutput(BaseModel):\n",
        "    value: float"
      ],
      "metadata": {
        "id": "AU98IjsFTRsT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ALLOWED_OPS = {\n",
        "    ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul,\n",
        "    ast.Div: op.truediv, ast.Pow: op.pow, ast.Mod: op.mod,\n",
        "    ast.USub: op.neg, ast.UAdd: op.pos,\n",
        "}\n",
        "\n",
        "def _safe_eval(node):\n",
        "    if isinstance(node, ast.Constant):\n",
        "        if isinstance(node.value, (int, float)): return node.value\n",
        "        raise ValueError(\"Only numeric constants allowed\")\n",
        "    if isinstance(node, ast.UnaryOp) and type(node.op) in _ALLOWED_OPS:\n",
        "        return _ALLOWED_OPS[type(node.op)](_safe_eval(node.operand))\n",
        "    if isinstance(node, ast.BinOp) and type(node.op) in _ALLOWED_OPS:\n",
        "        return _ALLOWED_OPS[type(node.op)](_safe_eval(node.left), _safe_eval(node.right))\n",
        "    if isinstance(node, ast.Expr): return _safe_eval(node.value)\n",
        "    raise ValueError(\"Unsupported expression\")\n"
      ],
      "metadata": {
        "id": "RCLDI1o1QCMh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.tool\n",
        "def calc_expr(ctx: RunContext[Deps], data: CalcInput) -> CalcOutput:\n",
        "    \"\"\"Safely evaluate arithmetic expression: + - * / ** % and parentheses.\n",
        "    Example: calc_expr(expression=\"3*(2+5)/7\")\n",
        "    \"\"\"\n",
        "    tree = ast.parse(data.expression, mode=\"eval\")\n",
        "    val = float(_safe_eval(tree.body))\n",
        "    ctx.deps.log.append({\"tool\": \"calc_expr\", \"expr\": data.expression, \"value\": val})\n",
        "    return CalcOutput(value=val)"
      ],
      "metadata": {
        "id": "FUkr9q30QDu_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2uG6ULCUSIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 2: Converter"
      ],
      "metadata": {
        "id": "qvjy0e6cSNwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertInput(BaseModel):\n",
        "    query: str  # \"<num> <unit> to <target unit>\"\n",
        "\n",
        "class ConvertOutput(BaseModel):\n",
        "    value: float\n",
        "    from_unit: str\n",
        "    to_unit: str"
      ],
      "metadata": {
        "id": "jg9WwMttTOlE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_UNIT_TO_BASE = {\n",
        "    # length -> meters\n",
        "    \"m\": (\"m\", 1.0), \"meter\": (\"m\", 1.0), \"meters\": (\"m\", 1.0),\n",
        "    \"cm\": (\"m\", 0.01), \"mm\": (\"m\", 0.001), \"km\": (\"m\", 1000.0),\n",
        "    \"inch\": (\"m\", 0.0254), \"in\": (\"m\", 0.0254), \"inches\": (\"m\", 0.0254),\n",
        "    \"ft\": (\"m\", 0.3048), \"foot\": (\"m\", 0.3048), \"feet\": (\"m\", 0.3048),\n",
        "    \"yd\": (\"m\", 0.9144), \"yard\": (\"m\", 0.9144), \"yards\": (\"m\", 0.9144),\n",
        "    # mass -> kilograms\n",
        "    \"kg\": (\"kg\", 1.0), \"g\": (\"kg\", 0.001),\n",
        "    \"lb\": (\"kg\", 0.45359237), \"lbs\": (\"kg\", 0.45359237),\n",
        "    \"pound\": (\"kg\", 0.45359237), \"pounds\": (\"kg\", 0.45359237),\n",
        "}\n",
        "\n",
        "def _parse_convert(q: str):\n",
        "    parts = q.strip().lower().split()\n",
        "    if \"to\" not in parts or len(parts) < 3:\n",
        "        raise ValueError(\"Use format like: '12 inches to cm'\")\n",
        "    to_idx = parts.index(\"to\")\n",
        "    num = float(parts[0].replace(\",\", \".\"))\n",
        "    from_unit = parts[1]; to_unit = parts[to_idx + 1]\n",
        "    return num, from_unit, to_unit"
      ],
      "metadata": {
        "id": "4cbcrYdYTfxs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.tool\n",
        "def convert_units(ctx: RunContext[Deps], data: ConvertInput) -> ConvertOutput:\n",
        "    \"\"\"Convert '<number> <from_unit> to <to_unit>' for length and mass.\n",
        "    Example: convert_units(query=\"12 inches to cm\")\n",
        "    \"\"\"\n",
        "    num, fu, tu = _parse_convert(data.query)\n",
        "    if fu not in _UNIT_TO_BASE or tu not in _UNIT_TO_BASE:\n",
        "        raise ValueError(\"Unsupported units\")\n",
        "    fam_f, kf = _UNIT_TO_BASE[fu]; fam_t, kt = _UNIT_TO_BASE[tu]\n",
        "    if fam_f != fam_t:\n",
        "        raise ValueError(f\"Incompatible unit families ({fu} -> {tu})\")\n",
        "    base_val = num * kf; out_val = base_val / kt\n",
        "    ctx.deps.log.append({\"tool\": \"convert_units\", \"query\": data.query, \"value\": out_val})\n",
        "    return ConvertOutput(value=out_val, from_unit=fu, to_unit=tu)"
      ],
      "metadata": {
        "id": "c3lsTEbLTgRd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 3: Evaluator\n",
        "\n",
        "Evaluation tool compares a numeric output with expected one, taking into account a margin of error"
      ],
      "metadata": {
        "id": "zrWzzml2VKOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EvalInput(BaseModel):\n",
        "    produced: str\n",
        "    expected: float\n",
        "    tol: float = 1e-6  # error margin\n",
        "\n",
        "class EvalOutput(BaseModel):\n",
        "    ok: bool\n",
        "    reason: str"
      ],
      "metadata": {
        "id": "bT_IwIa3VO5e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.tool\n",
        "def evaluate_nums(ctx: RunContext[Deps], data: EvalInput) -> EvalOutput:\n",
        "    \"\"\"Compare a numeric answer in 'produced' to 'expected' within tolerance.\"\"\"\n",
        "    import re\n",
        "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", data.produced.replace(\",\", \".\"))\n",
        "    if not m:\n",
        "        res = EvalOutput(ok=False, reason=\"no number found\")\n",
        "    else:\n",
        "        got = float(m.group(0))\n",
        "        res = EvalOutput(ok=abs(got - data.expected) <= data.tol, reason=f\"got={got}, expected={data.expected}\")\n",
        "    ctx.deps.log.append({\"tool\": \"evaluate_nums\", **res.model_dump()})\n",
        "    return res"
      ],
      "metadata": {
        "id": "qg2Ptk2NVWPo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 4: Letters counter"
      ],
      "metadata": {
        "id": "7TJBAo9z3Fg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "kK6-9xcE3E4T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharCountInput(BaseModel):\n",
        "    text: str\n",
        "    normalize: bool = True\n",
        "    letters_only: bool = True\n",
        "\n",
        "class CharCountPerWord(BaseModel):\n",
        "    word: str\n",
        "    counts: Dict[str, int]\n",
        "\n",
        "class CharCountOutput(BaseModel):\n",
        "    items: List[CharCountPerWord]"
      ],
      "metadata": {
        "id": "tDmux4RH3Ntm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _tokenize_words(text: str) -> List[str]:\n",
        "    return re.findall(r\"\\b\\w+\\b\", text, flags=re.UNICODE)\n",
        "\n",
        "def format_char_counts(out: CharCountOutput) -> str:\n",
        "    parts = []\n",
        "    for item in out.items:\n",
        "        counts_sorted = \", \".join(f\"{ch}:{n}\" for ch, n in sorted(item.counts.items()))\n",
        "        parts.append(f\"{item.word}:{{{counts_sorted}}}\")\n",
        "    return \", \".join(parts)"
      ],
      "metadata": {
        "id": "c4ht1eJ13aa1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@agent.tool\n",
        "def count_word_chars(ctx: RunContext[Deps], data: CharCountInput) -> CharCountOutput:\n",
        "    \"\"\"Count per-word character frequencies for the given text.\n",
        "    Examples:\n",
        "      count_word_chars(text=\"I'm strawberry\")\n",
        "      count_word_chars(text=\"I am a strawberry\")\n",
        "    Return a compact mapping per word.\n",
        "    \"\"\"\n",
        "    text = (data.text or \"\").strip()\n",
        "    if not text:\n",
        "        return CharCountOutput(items=[])\n",
        "    words = _tokenize_words(text)\n",
        "    items: List[CharCountPerWord] = []\n",
        "    for w in words:\n",
        "        word_key = w.lower() if data.normalize else w\n",
        "        letters = [\n",
        "            (ch.lower() if data.normalize else ch)\n",
        "            for ch in w\n",
        "            if (ch.isalpha() if data.letters_only else True)\n",
        "        ]\n",
        "        c = Counter(letters)\n",
        "        items.append(CharCountPerWord(word=word_key, counts=dict(c)))\n",
        "    ctx.deps.log.append({\"tool\": \"count_word_chars\", \"n_words\": len(items)})\n",
        "    return CharCountOutput(items=items)"
      ],
      "metadata": {
        "id": "DG7AMvLg3pSx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main agent.run triage"
      ],
      "metadata": {
        "id": "qnekC1efV0Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7gVW3v1JlTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16eec247-b67c-466a-af9f-f3c91f968fee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1580915630.py:2: RuntimeWarning: coroutine 'AbstractAgent.run' was never awaited\n",
            "  async def run(ctx: RunContext[Deps], question: str) -> Result:\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AayMmirD8Z1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}